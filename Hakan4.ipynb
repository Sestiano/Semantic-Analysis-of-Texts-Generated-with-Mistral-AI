{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d36bd7b",
   "metadata": {},
   "source": [
    "# Semantic Analysis of Texts Generated with Mistral AI\n",
    "\n",
    "This notebook demonstrates a complete pipeline for semantic analysis of texts generated using Mistral AI. The analysis includes:\n",
    "\n",
    "1. **Text Generation** - Using Mistral API to generate texts with different prompts and temperature values\n",
    "2. **Text Preprocessing** - Cleaning and preprocessing texts using SpaCy\n",
    "3. **Network Construction** - Building semantic networks using EmoAtlas\n",
    "4. **Network Analysis** - Calculating various network metrics and properties\n",
    "5. **Statistical Analysis** - Performing comprehensive statistical tests and advanced analysis\n",
    "6. **Visualization** - Creating visualizations for network analysis results\n",
    "\n",
    "The pipeline uses modular Python files that were extracted from the original Hakan.ipynb for better organization and reusability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883dac8",
   "metadata": {},
   "source": [
    "## 1. Import Required Modules\n",
    "\n",
    "First, we import all necessary Python modules and functions from the .py files present in the workspace. Each module serves a specific purpose in our analysis pipeline:\n",
    "\n",
    "- **MistralGenerator**: Handles text generation using Mistral AI API\n",
    "- **TextPreprocessor**: Cleans and preprocesses texts using SpaCy\n",
    "- **NetworkBuilder**: Constructs semantic networks using EmoAtlas\n",
    "- **NetworkAnalyzer**: Calculates network metrics and properties\n",
    "- **StatisticalAnalyzer**: Performs statistical tests and advanced analysis\n",
    "- **NetworkVisualizer**: Creates visualizations for the analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36166088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully!\n",
      "Custom modules available:\n",
      "  - MistralGenerator: Text generation with Mistral AI\n",
      "  - TextPreprocessor: Text cleaning and preprocessing\n",
      "  - NetworkBuilder: Semantic network construction\n",
      "  - NetworkAnalyzer: Network metrics calculation\n",
      "  - StatisticalAnalyzer: Advanced statistical analysis\n",
      "  - NetworkVisualizer: Data visualization\n"
     ]
    }
   ],
   "source": [
    "# Import our custom modules\n",
    "from mistral_generator import MistralGenerator\n",
    "from text_preprocessor import TextPreprocessor\n",
    "from network_builder import NetworkBuilder\n",
    "from network_analyzer import NetworkAnalyzer\n",
    "from statistical_tests import StatisticalAnalyzer\n",
    "from visualizer import NetworkVisualizer\n",
    "\n",
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(\"Custom modules available:\")\n",
    "print(\"  - MistralGenerator: Text generation with Mistral AI\")\n",
    "print(\"  - TextPreprocessor: Text cleaning and preprocessing\")\n",
    "print(\"  - NetworkBuilder: Semantic network construction\")\n",
    "print(\"  - NetworkAnalyzer: Network metrics calculation\")\n",
    "print(\"  - StatisticalAnalyzer: Advanced statistical analysis\")\n",
    "print(\"  - NetworkVisualizer: Data visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f636b2",
   "metadata": {},
   "source": [
    "## 2. Configuration and Setup\n",
    "\n",
    "This section provides centralized configuration for the entire analysis pipeline. All parameters are defined here and passed to the respective modules, ensuring consistent configuration across all components.\n",
    "\n",
    "**Key Configuration Areas:**\n",
    "- **Prompts**: Different types of prompts (complex and vague) to generate texts\n",
    "- **Temperature values**: Different creativity levels for text generation\n",
    "- **Directory structure**: Organization of input and output files\n",
    "- **Analysis parameters**: Settings for statistical tests and visualization\n",
    "\n",
    "**Important**: All Python modules have been cleaned to remove hardcoded parameters. Configuration is now centralized in this notebook, making it easier to modify parameters and maintain consistency across the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfb2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced configuration completed successfully!\n",
      "Temperature values: [0.001, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]\n",
      "Prompt types: ['complex', 'vague']\n",
      "Directory structure: {'texts': 'texts', 'results': 'results', 'figures': 'figures', 'visualizations': 'visualizations', 'bootstrap': 'bootstrap_results', 'semantic': 'semantic_analysis'}\n",
      "Enhanced analysis parameters:\n",
      "  - Text completions: 100\n",
      "  - Bootstrap samples: 1000\n",
      "  - Semantic models: ['bertscore', 'fasttext', 'sentence_transformers']\n",
      "  - BERTScore model: distilbert-base-uncased\n",
      "  - Similarity thresholds: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
      "  - Primary threshold: 0.3\n",
      "  - Temperature correlation methods: ['pearson', 'spearman', 'kendall']\n",
      "  - Community detection: ['louvain', 'leiden']\n",
      "  - Effect size measures: ['cohens_d', 'hedges_g', 'eta_squared']\n"
     ]
    }
   ],
   "source": [
    "# CENTRALIZED CONFIGURATION - All parameters are defined here\n",
    "# The Python modules have been cleaned to remove hardcoded parameters\n",
    "# This ensures consistent configuration across the entire pipeline\n",
    "\n",
    "# Temperature values for text generation\n",
    "TEMPERATURES = [0.001, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]\n",
    "\n",
    "# Define prompts for text generation\n",
    "PROMPTS = {\n",
    "    'complex': '''In the shadowed alley of a forgotten district, where cobblestones whisper secrets \n",
    "    of centuries past, an antique music box sits abandoned on a crumbling windowsill. Its once-golden \n",
    "    surface now tarnished with age, the delicate ballerina inside frozen mid-pirouette. When the wind \n",
    "    catches just right, it emits a haunting melody that draws the curious and the lost. Tonight, \n",
    "    three strangers—a weary traveler, a local historian, and a child who shouldn't be out so late—\n",
    "    find themselves inexplicably drawn to its melancholic song. As they approach, the music box \n",
    "    begins to glow with an ethereal light, and each person sees something different reflected in \n",
    "    its mirror: a memory, a possibility, or perhaps a warning from another time.''',\n",
    "    \n",
    "    'vague': '''A person finds something unexpected in an old building and decides to investigate further.'''\n",
    "}\n",
    "\n",
    "# Directory structure for organizing files\n",
    "DIRS = {\n",
    "    'texts': 'texts',\n",
    "    'results': 'results',\n",
    "    'figures': 'figures',\n",
    "    'visualizations': 'visualizations',\n",
    "    'bootstrap': 'bootstrap_results',\n",
    "    'semantic': 'semantic_analysis'\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in DIRS.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Enhanced analysis parameters for robust statistical analysis\n",
    "ANALYSIS_CONFIG = {\n",
    "    # Text generation parameters\n",
    "    'n_completions': 100,  # Increased for statistical robustness\n",
    "    \n",
    "    # Statistical analysis parameters\n",
    "    'alpha': 0.05,  # significance level for statistical tests\n",
    "    'fdr_alpha': 0.05,  # False Discovery Rate correction\n",
    "    \n",
    "    # Bootstrap parameters for robust confidence intervals\n",
    "    'n_bootstrap': 1000,  # Number of bootstrap samples\n",
    "    'bootstrap_ci': 0.95,  # Confidence interval level\n",
    "    'bootstrap_method': 'percentile',  # Bootstrap CI method\n",
    "    \n",
    "    # Enhanced semantic analysis parameters with BERTScore\n",
    "    'semantic_models': ['bertscore', 'fasttext', 'sentence_transformers'],\n",
    "    'bertscore_model': 'distilbert-base-uncased',  # High-performance BERT model\n",
    "    'bertscore_lang': 'en',  # Language for BERTScore\n",
    "    'bertscore_rescale': True,  # Rescale scores for better interpretability\n",
    "    'embedding_dim': 300,  # Dimension for other embeddings\n",
    "    \n",
    "    # Multiple similarity thresholds for comprehensive analysis\n",
    "    'similarity_threshold': 0.3,  # Primary threshold (reduced from 0.7)\n",
    "    'similarity_thresholds': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7],  # Multiple thresholds for analysis\n",
    "    \n",
    "    # Topic modeling with enhanced parameters\n",
    "    'topic_modeling': True,  # Enable topic modeling analysis\n",
    "    'n_topics': 10,  # Number of topics for LDA\n",
    "    'topic_coherence_models': ['c_v', 'c_npmi', 'c_uci'],  # Multiple coherence measures\n",
    "    \n",
    "    # Enhanced temperature analysis\n",
    "    'temperature_analysis': {\n",
    "        'min_pairs_per_temp': 3,  # Minimum pairs needed for reliable analysis\n",
    "        'temperature_bins': 5,  # Number of bins for temperature analysis\n",
    "        'correlation_methods': ['pearson', 'spearman', 'kendall']  # Multiple correlation methods\n",
    "    },\n",
    "    \n",
    "    # Network analysis enhancements\n",
    "    'motif_analysis': True,  # Enable network motif analysis\n",
    "    'community_detection': ['louvain', 'leiden'],  # Community detection algorithms\n",
    "    'centrality_measures': ['degree', 'betweenness', 'closeness', 'eigenvector', 'pagerank'],\n",
    "    \n",
    "    # Visualization parameters\n",
    "    'figsize': (12, 8),\n",
    "    'style': 'seaborn-v0_8',\n",
    "    'dpi': 300,  # High resolution for publication\n",
    "    \n",
    "    # Effect size calculations\n",
    "    'effect_size_measures': ['cohens_d', 'hedges_g', 'eta_squared'],\n",
    "    \n",
    "    # Advanced statistical tests\n",
    "    'permutation_tests': True,\n",
    "    'n_permutations': 10000,\n",
    "    'mixed_effects': True,  # Enable mixed-effects modeling\n",
    "}\n",
    "\n",
    "print(\"Enhanced configuration completed successfully!\")\n",
    "print(f\"Temperature values: {TEMPERATURES}\")\n",
    "print(f\"Prompt types: {list(PROMPTS.keys())}\")\n",
    "print(f\"Directory structure: {DIRS}\")\n",
    "print(f\"Enhanced analysis parameters:\")\n",
    "print(f\"  - Text completions: {ANALYSIS_CONFIG['n_completions']}\")\n",
    "print(f\"  - Bootstrap samples: {ANALYSIS_CONFIG['n_bootstrap']}\")\n",
    "print(f\"  - Semantic models: {ANALYSIS_CONFIG['semantic_models']}\")\n",
    "print(f\"  - BERTScore model: {ANALYSIS_CONFIG['bertscore_model']}\")\n",
    "print(f\"  - Similarity thresholds: {ANALYSIS_CONFIG['similarity_thresholds']}\")\n",
    "print(f\"  - Primary threshold: {ANALYSIS_CONFIG['similarity_threshold']}\")\n",
    "print(f\"  - Temperature correlation methods: {ANALYSIS_CONFIG['temperature_analysis']['correlation_methods']}\")\n",
    "print(f\"  - Community detection: {ANALYSIS_CONFIG['community_detection']}\")\n",
    "print(f\"  - Effect size measures: {ANALYSIS_CONFIG['effect_size_measures']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc937546",
   "metadata": {},
   "source": [
    "## 3. Text Generation (Optional)\n",
    "\n",
    "This section demonstrates how to use the `MistralGenerator` class to generate new texts. Since the workspace already contains generated texts, this step is optional but shows how the original texts were created.\n",
    "\n",
    "The `MistralGenerator` class handles:\n",
    "- API authentication with Mistral AI\n",
    "- Text generation with different temperature values\n",
    "- Systematic organization of generated texts\n",
    "- Multiple completions per prompt-temperature combination\n",
    "\n",
    "**Note**: To run this section, you need a valid Mistral API key. If you don't have one, you can skip this section and proceed with the existing generated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d0b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text generation...\n",
      "Generating 100 completions per condition...\n",
      "Total conditions: 2 prompts × 7 temperatures = 14\n",
      "Starting generation of 14 text sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vague T=1.5: 100%|██████████| 14/14 [12:50<00:00, 55.04s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation completed!\n",
      "  - Processed prompts: 2\n",
      "  - Temperatures per prompt: 7\n",
      "  - Completions per temperature: 100\n",
      "  - Total texts generated: 1400\n",
      "Text generation completed!\n",
      "Text generation failed. Check your API key and try again.\n",
      "No existing generated texts found. You may need to run the text generation step.\n",
      "Make sure you have a valid Mistral API key and uncomment the code above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional: Generate new texts using Mistral AI\n",
    "# To generate new texts, follow these steps:\n",
    "\n",
    "# Step 1: Install required packages (run in terminal if needed)\n",
    "# pip install mistralai\n",
    "\n",
    "# Step 2: Get your Mistral API key from https://console.mistral.ai/\n",
    "# Replace \"your_api_key_here\" with your actual API key\n",
    "\n",
    "# Step 3: Run the following code\n",
    "mistral_api_key = \"insert_your_API_KEY_here\"  # Replace with your actual API key\n",
    "\n",
    "# Initialize Mistral generator\n",
    "generator = MistralGenerator(api_key=mistral_api_key)\n",
    "\n",
    "# Generate texts for all prompt-temperature combinations\n",
    "print(\"Starting text generation...\")\n",
    "print(f\"Generating {ANALYSIS_CONFIG['n_completions']} completions per condition...\")\n",
    "print(f\"Total conditions: {len(PROMPTS)} prompts × {len(TEMPERATURES)} temperatures = {len(PROMPTS) * len(TEMPERATURES)}\")\n",
    "\n",
    "generator.generate_texts(\n",
    "    prompts=PROMPTS,\n",
    "    temperatures=TEMPERATURES,\n",
    "    n_completions=ANALYSIS_CONFIG['n_completions'],\n",
    "    texts_dir=DIRS['texts']\n",
    ")\n",
    "\n",
    "print(\"Text generation completed!\")\n",
    "\n",
    "# Check existing generated texts - Updated to match existing directory structure\n",
    "complex_dir = os.path.join(DIRS['texts'], 'mistral_complex')\n",
    "vague_dir = os.path.join(DIRS['texts'], 'mistral_vague')\n",
    "\n",
    "if os.path.exists(complex_dir) and os.path.exists(vague_dir):\n",
    "    complex_files = len(glob.glob(os.path.join(complex_dir, '*.txt')))\n",
    "    vague_files = len(glob.glob(os.path.join(vague_dir, '*.txt')))\n",
    "    print(f\"Found existing generated texts:\")\n",
    "    print(f\"  - Complex prompts: {complex_files} files\")\n",
    "    print(f\"  - Vague prompts: {vague_files} files\")\n",
    "    print(f\"  - Total files: {complex_files + vague_files}\")\n",
    "else:\n",
    "    print(\"No existing generated texts found. You may need to run the text generation step.\")\n",
    "    print(\"Make sure you have a valid Mistral API key and uncomment the code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca041d7",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing\n",
    "\n",
    "The `TextPreprocessor` class handles text cleaning and preprocessing using SpaCy. This step is crucial for preparing raw texts for network analysis. The preprocessing includes:\n",
    "\n",
    "- **Tokenization**: Breaking text into individual words\n",
    "- **Stop word removal**: Removing common words that don't contribute to meaning\n",
    "- **Punctuation removal**: Cleaning special characters\n",
    "- **Lemmatization**: Reducing words to their base forms\n",
    "- **Normalization**: Converting to lowercase and handling special characters\n",
    "\n",
    "The preprocessed texts are saved in separate directories for organized storage and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bdfd2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing text preprocessor...\n",
      "Starting text preprocessing...\n",
      "Processing complex prompts...\n",
      "Starting text preprocessing...\n",
      "Processing complex prompts...\n",
      "7 files processed for complex prompts\n",
      "Processing vague prompts...\n",
      "7 files processed for complex prompts\n",
      "Processing vague prompts...\n",
      "7 files processed for vague prompts\n",
      "Text preprocessing completed! Total files processed: 14\n",
      "  - Complex cleaned files: 7\n",
      "  - Vague cleaned files: 7\n",
      "7 files processed for vague prompts\n",
      "Text preprocessing completed! Total files processed: 14\n",
      "  - Complex cleaned files: 7\n",
      "  - Vague cleaned files: 7\n"
     ]
    }
   ],
   "source": [
    "# Initialize text preprocessor\n",
    "print(\"Initializing text preprocessor...\")\n",
    "preprocessor = TextPreprocessor(lang_model='en_core_web_lg')\n",
    "\n",
    "# Define source and target directories - Fixed to match existing directory structure\n",
    "source_dirs = [\n",
    "    os.path.join(DIRS['texts'], 'mistral_complex'),\n",
    "    os.path.join(DIRS['texts'], 'mistral_vague')\n",
    "]\n",
    "\n",
    "target_dirs = [\n",
    "    os.path.join(DIRS['texts'], 'cleaned_mistral_complex'),\n",
    "    os.path.join(DIRS['texts'], 'cleaned_mistral_vague')\n",
    "]\n",
    "\n",
    "# Check if preprocessing has already been done\n",
    "already_processed = True\n",
    "for target_dir in target_dirs:\n",
    "    if not os.path.exists(target_dir) or len(os.listdir(target_dir)) == 0:\n",
    "        already_processed = False\n",
    "        break\n",
    "\n",
    "if already_processed:\n",
    "    print(\"Text preprocessing already completed!\")\n",
    "    for i, (source_dir, target_dir) in enumerate(zip(source_dirs, target_dirs)):\n",
    "        if os.path.exists(source_dir) and os.path.exists(target_dir):\n",
    "            source_files = len(glob.glob(os.path.join(source_dir, '*.txt')))\n",
    "            target_files = len(glob.glob(os.path.join(target_dir, '*.txt')))\n",
    "            prompt_type = 'complex' if 'complex' in source_dir else 'vague'\n",
    "            print(f\"  - {prompt_type.capitalize()} prompts: {source_files} → {target_files} files\")\n",
    "else:\n",
    "    print(\"Starting text preprocessing...\")\n",
    "    \n",
    "    # Process texts for each prompt type using individual file processing\n",
    "    total_processed = 0\n",
    "    \n",
    "    for source_dir, target_dir in zip(source_dirs, target_dirs):\n",
    "        if os.path.exists(source_dir):\n",
    "            prompt_type = 'complex' if 'complex' in source_dir else 'vague'\n",
    "            print(f\"Processing {prompt_type} prompts...\")\n",
    "            \n",
    "            # Create target directory if it doesn't exist\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            \n",
    "            # Get all text files in source directory\n",
    "            text_files = glob.glob(os.path.join(source_dir, '*.txt'))\n",
    "            \n",
    "            processed_count = 0\n",
    "            for text_file in text_files:\n",
    "                filename = os.path.basename(text_file)\n",
    "                # Create output filename (remove _cleaned suffix to avoid duplication)\n",
    "                base_name = filename.replace('_cleaned.txt', '.txt').replace('.txt', '')\n",
    "                output_file = os.path.join(target_dir, f\"{base_name}_cleaned.txt\")\n",
    "                \n",
    "                # Process the file\n",
    "                if preprocessor.clean_single_file(text_file, output_file):\n",
    "                    processed_count += 1\n",
    "                    \n",
    "            total_processed += processed_count\n",
    "            print(f\"{processed_count} files processed for {prompt_type} prompts\")\n",
    "        else:\n",
    "            print(f\"Source directory not found: {source_dir}\")\n",
    "    \n",
    "    print(f\"Text preprocessing completed! Total files processed: {total_processed}\")\n",
    "    \n",
    "    # Display summary\n",
    "    for target_dir in target_dirs:\n",
    "        if os.path.exists(target_dir):\n",
    "            processed_files = len(glob.glob(os.path.join(target_dir, '*.txt')))\n",
    "            prompt_type = 'complex' if 'complex' in target_dir else 'vague'\n",
    "            print(f\"  - {prompt_type.capitalize()} cleaned files: {processed_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc94a5c",
   "metadata": {},
   "source": [
    "## 5. Semantic Network Construction\n",
    "\n",
    "The `NetworkBuilder` class constructs semantic networks from the preprocessed texts using EmoAtlas. This step transforms textual data into network representations where:\n",
    "\n",
    "- **Nodes**: Represent words or concepts from the texts\n",
    "- **Edges**: Represent semantic relationships between words\n",
    "- **Weights**: Indicate the strength of relationships\n",
    "\n",
    "EmoAtlas provides emotional and semantic associations between words, allowing us to build meaningful networks that capture the underlying structure of the generated texts. The networks are built for each prompt type and temperature combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b771f403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network builder...\n",
      "Building semantic networks from preprocessed texts...\n",
      "Found 14 files to process with EmoAtlas...\n",
      "Building semantic networks from preprocessed texts...\n",
      "Found 14 files to process with EmoAtlas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vague_1.0.txt: 100%|██████████| 14/14 [1:04:37<00:00, 277.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EmoAtlas network creation completed!\n",
      "  - Edge lists saved in: emo_edges_complex/ and emo_edges_vague/\n",
      "Semantic network construction completed!\n",
      "  - Complex networks: 7 files created\n",
      "  - Vague networks: 7 files created\n",
      "\n",
      "Edge list files contain network connections:\n",
      "  - Complex example: emo_edge_list_complex_0.25.txt\n",
      "  - Vague example: emo_edge_list_vague_1.0.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize network builder\n",
    "print(\"Initializing network builder...\")\n",
    "network_builder = NetworkBuilder()\n",
    "\n",
    "# Check if networks have already been built\n",
    "edge_dirs = ['emo_edges_complex', 'emo_edges_vague']\n",
    "networks_exist = all(os.path.exists(edge_dir) and len(os.listdir(edge_dir)) > 0 for edge_dir in edge_dirs)\n",
    "\n",
    "if networks_exist:\n",
    "    print(\"Semantic networks already constructed!\")\n",
    "    for edge_dir in edge_dirs:\n",
    "        if os.path.exists(edge_dir):\n",
    "            edge_files = len(glob.glob(os.path.join(edge_dir, '*.txt')))\n",
    "            network_type = 'complex' if 'complex' in edge_dir else 'vague'\n",
    "            print(f\"  - {network_type.capitalize()} networks: {edge_files} files\")\n",
    "else:\n",
    "    print(\"Building semantic networks from preprocessed texts...\")\n",
    "    \n",
    "    # Build networks from preprocessed texts - Updated to match existing directory structure\n",
    "    source_dirs = [\n",
    "        os.path.join(DIRS['texts'], 'mistral_complex'),\n",
    "        os.path.join(DIRS['texts'], 'mistral_vague')\n",
    "    ]\n",
    "    \n",
    "    network_builder.build_networks_from_texts(\n",
    "        texts_dir=DIRS['texts'],\n",
    "        source_dirs=source_dirs\n",
    "    )\n",
    "    \n",
    "    print(\"Semantic network construction completed!\")\n",
    "    \n",
    "    # Display results\n",
    "    for edge_dir in edge_dirs:\n",
    "        if os.path.exists(edge_dir):\n",
    "            edge_files = len(glob.glob(os.path.join(edge_dir, '*.txt')))\n",
    "            network_type = 'complex' if 'complex' in edge_dir else 'vague'\n",
    "            print(f\"  - {network_type.capitalize()} networks: {edge_files} files created\")\n",
    "\n",
    "# Show example of edge list files\n",
    "print(\"\\nEdge list files contain network connections:\")\n",
    "for edge_dir in edge_dirs:\n",
    "    if os.path.exists(edge_dir):\n",
    "        sample_files = glob.glob(os.path.join(edge_dir, '*.txt'))\n",
    "        if sample_files:\n",
    "            sample_file = sample_files[0]\n",
    "            network_type = 'complex' if 'complex' in edge_dir else 'vague'\n",
    "            print(f\"  - {network_type.capitalize()} example: {os.path.basename(sample_file)}\")\n",
    "        else:\n",
    "            network_type = 'complex' if 'complex' in edge_dir else 'vague'\n",
    "            print(f\"  - {network_type.capitalize()}: No files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b6943",
   "metadata": {},
   "source": [
    "## 6. Network Analysis\n",
    "\n",
    "The `NetworkAnalyzer` class calculates various network metrics and properties for each constructed network. This analysis provides quantitative measures of network structure, including:\n",
    "\n",
    "- **Basic metrics**: Number of nodes, edges, density\n",
    "- **Connectivity**: Connected components, clustering coefficient\n",
    "- **Centrality measures**: Degree, betweenness, closeness, eigenvector centrality\n",
    "- **Path metrics**: Average path length, diameter\n",
    "- **Community structure**: Modularity, community detection\n",
    "\n",
    "These metrics help us understand the semantic structure of texts generated under different conditions (prompt type and temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757d4d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network analyzer...\n",
      "Network analysis already completed!\n",
      "  - Total networks analyzed: 14\n",
      "  - Prompt types: ['complex' 'vague']\n",
      "  - Temperature values: [0.001, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]\n",
      "  - Metrics calculated: 11 metrics per network\n",
      "\n",
      "Sample of network metrics:\n",
      "  prompt_type  temperature   density\n",
      "0     complex         0.25  0.021817\n",
      "1     complex         0.50  0.020250\n",
      "2     complex         1.25  0.018980\n",
      "3     complex         1.50  0.018121\n",
      "4     complex         0.75  0.020179\n"
     ]
    }
   ],
   "source": [
    "# Initialize network analyzer\n",
    "print(\"Initializing network analyzer...\")\n",
    "network_analyzer = NetworkAnalyzer()\n",
    "\n",
    "# Check if analysis has already been performed\n",
    "results_file = os.path.join(DIRS['results'], 'network_metrics.csv')\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    print(\"Network analysis already completed!\")\n",
    "    df = pd.read_csv(results_file)\n",
    "    print(f\"  - Total networks analyzed: {len(df)}\")\n",
    "    print(f\"  - Prompt types: {df['prompt_type'].unique()}\")\n",
    "    print(f\"  - Temperature values: {sorted(df['temperature'].unique())}\")\n",
    "    print(f\"  - Metrics calculated: {len(df.columns) - 3} metrics per network\")  # excluding filename, prompt_type, temperature\n",
    "else:\n",
    "    print(\"Starting network analysis...\")\n",
    "    \n",
    "    # Get all edge list files\n",
    "    edge_dirs = ['emo_edges_complex', 'emo_edges_vague']\n",
    "    all_results = []\n",
    "    \n",
    "    for edge_dir in edge_dirs:\n",
    "        if os.path.exists(edge_dir):\n",
    "            edge_files = glob.glob(os.path.join(edge_dir, '*.txt'))\n",
    "            prompt_type = 'complex' if 'complex' in edge_dir else 'vague'\n",
    "            \n",
    "            print(f\"  Analyzing {prompt_type} networks...\")\n",
    "            \n",
    "            for edge_file in edge_files:\n",
    "                # Analyze the network\n",
    "                result = network_analyzer.analyze_edge_list(edge_file)\n",
    "                if result:\n",
    "                    all_results.append(result)\n",
    "                    print(f\"    ✓ {os.path.basename(edge_file)}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Save results\n",
    "        df.to_csv(results_file, index=False)\n",
    "        print(f\"Network analysis completed! Results saved to {results_file}\")\n",
    "        print(f\"  - Total networks analyzed: {len(df)}\")\n",
    "        print(f\"  - Metrics calculated: {len(df.columns) - 3} metrics per network\")\n",
    "    else:\n",
    "        print(\"No networks found to analyze. Please check edge list files.\")\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "# Display sample of results\n",
    "if not df.empty:\n",
    "    print(\"\\nSample of network metrics:\")\n",
    "    display_cols = ['filename', 'prompt_type', 'temperature', 'num_nodes', 'num_edges', 'density', 'clustering_coefficient']\n",
    "    available_cols = [col for col in display_cols if col in df.columns]\n",
    "    print(df[available_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b78b1",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis\n",
    "\n",
    "The `StatisticalAnalyzer` class performs comprehensive statistical analysis of the network metrics. This advanced analysis includes:\n",
    "\n",
    "- **Normality testing**: Shapiro-Wilk, Jarque-Bera, Anderson-Darling tests\n",
    "- **Comparative analysis**: Mann-Whitney U tests, Kruskal-Wallis tests\n",
    "- **Correlation analysis**: Pearson and Spearman correlations\n",
    "- **Dimensionality reduction**: Principal Component Analysis (PCA)\n",
    "- **Clustering analysis**: K-means clustering with validation metrics\n",
    "- **Regression analysis**: Linear and Random Forest regression models\n",
    "\n",
    "This analysis helps us understand the statistical relationships between different conditions (prompt type, temperature) and network properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2dafe",
   "metadata": {},
   "source": [
    "## 7.1. Advanced Semantic Analysis with BERTScore\n",
    "\n",
    "This section implements comprehensive semantic analysis using state-of-the-art transformer-based models and advanced NLP techniques. The analysis includes:\n",
    "\n",
    "- **BERTScore**: Contextual embeddings from BERT models for semantic similarity evaluation\n",
    "- **FastText**: Sub-word embeddings for robust semantic representation\n",
    "- **Sentence Transformers**: Sentence-level semantic similarity analysis\n",
    "- **Topic Modeling**: Latent Dirichlet Allocation (LDA) for thematic analysis\n",
    "- **Semantic Similarity Networks**: Networks based on transformer embeddings rather than just EmoAtlas\n",
    "- **Coherence Analysis**: Measuring semantic coherence across different temperature settings using BERTScore\n",
    "- **Semantic Drift**: Analyzing how semantic content changes with temperature variations\n",
    "\n",
    "**BERTScore Advantages:**\n",
    "- Contextual understanding of word meaning\n",
    "- Better handling of synonyms and paraphrases\n",
    "- More robust to word order variations\n",
    "- State-of-the-art performance on semantic similarity tasks\n",
    "\n",
    "This transformer-based approach provides a more sophisticated understanding of how generation parameters affect semantic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5ee7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADVANCED SEMANTIC ANALYSIS WITH BERTSCORE\n",
      "Loading texts for semantic analysis...\n",
      "Loaded 14 texts for semantic analysis\n",
      "Computing BERTScore similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 91 BERTScore similarity pairs\n",
      "BERTScore results saved to semantic_analysis/bertscore_similarities.csv\n",
      "Analyzing semantic coherence patterns...\n",
      "\n",
      "ENHANCED SEMANTIC COHERENCE ANALYSIS:\n",
      "Same prompt type coherence:\n",
      "                  mean       std  count    median       min       max\n",
      "text1_prompt                                                         \n",
      "complex       0.864552  0.011658     21  0.861355  0.846952  0.892121\n",
      "vague         0.866313  0.010609     21  0.869610  0.847549  0.880628\n",
      "\n",
      "Cross prompt type coherence:\n",
      "  Mean F1: 0.8253 ± 0.0053\n",
      "  Median F1: 0.8253\n",
      "  Range: [0.8129, 0.8377]\n",
      "\n",
      "Temperature-based coherence (same temperature pairs):\n",
      "   temperature   mean_f1  std_f1  count\n",
      "0        0.001  0.820467     NaN      1\n",
      "1        0.250  0.825259     NaN      1\n",
      "2        0.500  0.831417     NaN      1\n",
      "3        0.750  0.827367     NaN      1\n",
      "4        1.000  0.827270     NaN      1\n",
      "5        1.250  0.828069     NaN      1\n",
      "6        1.500  0.826234     NaN      1\n",
      "\n",
      "Temperature correlation analysis:\n",
      "  pearson: r = 0.0047, p = 0.9645 n.s.\n",
      "  spearman: r = -0.0014, p = 0.9893 n.s.\n",
      "  kendall: r = 0.0000, p = 1.0000 n.s.\n",
      "Performing topic modeling analysis...\n",
      "\n",
      "ENHANCED TOPIC MODELING RESULTS:\n",
      "Identified 10 topics\n",
      "Topic distribution by prompt type and temperature:\n",
      "  prompt_type  temperature  dominant_topic  count\n",
      "0     complex        0.001               5      1\n",
      "1     complex        0.250               5      1\n",
      "2     complex        0.500               5      1\n",
      "3     complex        0.750               5      1\n",
      "4     complex        1.000               5      1\n",
      "5     complex        1.250               5      1\n",
      "6     complex        1.500               5      1\n",
      "7       vague        0.001               7      1\n",
      "8       vague        0.250               7      1\n",
      "9       vague        0.500               7      1\n",
      "\n",
      "Topic diversity (entropy) by conditions:\n",
      "   prompt_type  temperature      mean  std\n",
      "0      complex        0.001  0.449429  NaN\n",
      "1      complex        0.250  0.450593  NaN\n",
      "2      complex        0.500  0.465836  NaN\n",
      "3      complex        0.750  0.483305  NaN\n",
      "4      complex        1.000  0.483986  NaN\n",
      "5      complex        1.250  0.496720  NaN\n",
      "6      complex        1.500  0.520158  NaN\n",
      "7        vague        0.001  0.357730  NaN\n",
      "8        vague        0.250  0.328827  NaN\n",
      "9        vague        0.500  0.319306  NaN\n",
      "10       vague        0.750  0.316328  NaN\n",
      "11       vague        1.000  0.309393  NaN\n",
      "12       vague        1.250  0.985112  NaN\n",
      "13       vague        1.500  0.310532  NaN\n",
      "Analyzing networks at multiple similarity thresholds...\n",
      "Error calculating enhanced metrics for threshold 0.25: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.25: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.3: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.3: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.35: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.35: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.4: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.4: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.45: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.45: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.5: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.5: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.6: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.6: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.7: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.7: 91 edges, density 1.000\n",
      "\n",
      "MULTI-THRESHOLD NETWORK ANALYSIS:\n",
      "   threshold  n_edges  density  n_components  avg_clustering\n",
      "0       0.25       91      1.0             1             1.0\n",
      "1       0.30       91      1.0             1             1.0\n",
      "2       0.35       91      1.0             1             1.0\n",
      "3       0.40       91      1.0             1             1.0\n",
      "4       0.45       91      1.0             1             1.0\n",
      "5       0.50       91      1.0             1             1.0\n",
      "6       0.60       91      1.0             1             1.0\n",
      "7       0.70       91      1.0             1             1.0\n",
      "\n",
      "Optimal threshold for analysis: 0.7\n",
      "OPTIMAL SEMANTIC SIMILARITY NETWORK (threshold=0.7):\n",
      "  n_nodes: 14\n",
      "  n_edges: 91\n",
      "  density: 1.0\n",
      "  avg_clustering: 1.0\n",
      "  n_components: 1\n",
      "  largest_component_size: 14\n",
      "  avg_path_length_largest_cc: 1.0\n",
      "Creating enhanced visualizations...\n",
      "\n",
      "ENHANCED TOPIC MODELING RESULTS:\n",
      "Identified 10 topics\n",
      "Topic distribution by prompt type and temperature:\n",
      "  prompt_type  temperature  dominant_topic  count\n",
      "0     complex        0.001               5      1\n",
      "1     complex        0.250               5      1\n",
      "2     complex        0.500               5      1\n",
      "3     complex        0.750               5      1\n",
      "4     complex        1.000               5      1\n",
      "5     complex        1.250               5      1\n",
      "6     complex        1.500               5      1\n",
      "7       vague        0.001               7      1\n",
      "8       vague        0.250               7      1\n",
      "9       vague        0.500               7      1\n",
      "\n",
      "Topic diversity (entropy) by conditions:\n",
      "   prompt_type  temperature      mean  std\n",
      "0      complex        0.001  0.449429  NaN\n",
      "1      complex        0.250  0.450593  NaN\n",
      "2      complex        0.500  0.465836  NaN\n",
      "3      complex        0.750  0.483305  NaN\n",
      "4      complex        1.000  0.483986  NaN\n",
      "5      complex        1.250  0.496720  NaN\n",
      "6      complex        1.500  0.520158  NaN\n",
      "7        vague        0.001  0.357730  NaN\n",
      "8        vague        0.250  0.328827  NaN\n",
      "9        vague        0.500  0.319306  NaN\n",
      "10       vague        0.750  0.316328  NaN\n",
      "11       vague        1.000  0.309393  NaN\n",
      "12       vague        1.250  0.985112  NaN\n",
      "13       vague        1.500  0.310532  NaN\n",
      "Analyzing networks at multiple similarity thresholds...\n",
      "Error calculating enhanced metrics for threshold 0.25: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.25: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.3: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.3: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.35: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.35: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.4: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.4: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.45: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.45: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.5: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.5: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.6: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.6: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.7: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.7: 91 edges, density 1.000\n",
      "\n",
      "MULTI-THRESHOLD NETWORK ANALYSIS:\n",
      "   threshold  n_edges  density  n_components  avg_clustering\n",
      "0       0.25       91      1.0             1             1.0\n",
      "1       0.30       91      1.0             1             1.0\n",
      "2       0.35       91      1.0             1             1.0\n",
      "3       0.40       91      1.0             1             1.0\n",
      "4       0.45       91      1.0             1             1.0\n",
      "5       0.50       91      1.0             1             1.0\n",
      "6       0.60       91      1.0             1             1.0\n",
      "7       0.70       91      1.0             1             1.0\n",
      "\n",
      "Optimal threshold for analysis: 0.7\n",
      "OPTIMAL SEMANTIC SIMILARITY NETWORK (threshold=0.7):\n",
      "  n_nodes: 14\n",
      "  n_edges: 91\n",
      "  density: 1.0\n",
      "  avg_clustering: 1.0\n",
      "  n_components: 1\n",
      "  largest_component_size: 14\n",
      "  avg_path_length_largest_cc: 1.0\n",
      "Creating enhanced visualizations...\n",
      "Enhanced visualizations saved to: semantic_analysis/comprehensive_semantic_analysis.png\n",
      "\n",
      "ENHANCED SEMANTIC ANALYSIS COMPLETED SUCCESSFULLY!\n",
      "Results saved in: semantic_analysis\n",
      "Key findings:\n",
      "  - Complex prompts show -0.2% higher coherence\n",
      "  - Temperature effects: ['pearson', 'spearman', 'kendall'] correlations computed\n",
      "  - Network analysis: 8 thresholds analyzed\n",
      "  - Topic modeling: 10 topics identified with diversity analysis\n",
      "Enhanced visualizations saved to: semantic_analysis/comprehensive_semantic_analysis.png\n",
      "\n",
      "ENHANCED SEMANTIC ANALYSIS COMPLETED SUCCESSFULLY!\n",
      "Results saved in: semantic_analysis\n",
      "Key findings:\n",
      "  - Complex prompts show -0.2% higher coherence\n",
      "  - Temperature effects: ['pearson', 'spearman', 'kendall'] correlations computed\n",
      "  - Network analysis: 8 thresholds analyzed\n",
      "  - Topic modeling: 10 topics identified with diversity analysis\n"
     ]
    }
   ],
   "source": [
    "# Enhanced semantic analysis with BERTScore\n",
    "print(\"ADVANCED SEMANTIC ANALYSIS WITH BERTSCORE\")\n",
    "\n",
    "# Configure enhanced analysis parameters\n",
    "ANALYSIS_CONFIG.update({\n",
    "    'similarity_thresholds': [0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.60, 0.70],  # More granular thresholds\n",
    "    'temperature_analysis': {\n",
    "        'min_pairs_per_temp': 1,  # Minimum pairs needed per temperature\n",
    "        'temperature_bins': 5,    # Number of bins for temperature difference analysis\n",
    "        'correlation_methods': ['pearson', 'spearman', 'kendall']\n",
    "    },\n",
    "    'n_topics': 10,\n",
    "    'alpha': 0.05\n",
    "})\n",
    "\n",
    "def load_texts_for_semantic_analysis():\n",
    "    \"\"\"Load all texts for semantic analysis\"\"\"\n",
    "    texts = []\n",
    "    metadata = []\n",
    "    \n",
    "    # Load all text files from both prompt types\n",
    "    for prompt_type in ['complex', 'vague']:\n",
    "        for temp in TEMPERATURES:\n",
    "            file_path = os.path.join(DIRS['texts'], f'mistral_{prompt_type}', f'{prompt_type}_{temp}.txt')\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().strip()\n",
    "                    texts.append(text)\n",
    "                    metadata.append({\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'temperature': temp,\n",
    "                        'filename': f'{prompt_type}_{temp}.txt',\n",
    "                        'text_idx': len(texts) - 1\n",
    "                    })\n",
    "    \n",
    "    return texts, metadata\n",
    "\n",
    "def compute_bertscore_similarities(texts, metadata):\n",
    "    \"\"\"Compute BERTScore similarities between all pairs of texts\"\"\"\n",
    "    print(\"Computing BERTScore similarities...\")\n",
    "    \n",
    "    # Initialize BERTScore\n",
    "    from bert_score import score\n",
    "    \n",
    "    bertscore_results = []\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            # Compute BERTScore\n",
    "            P, R, F1 = score([texts[i]], [texts[j]], lang='en', verbose=False)\n",
    "            \n",
    "            # Extract metadata\n",
    "            meta1, meta2 = metadata[i], metadata[j]\n",
    "            \n",
    "            # Create result record with enhanced features\n",
    "            result = {\n",
    "                'text1_idx': i,\n",
    "                'text2_idx': j,\n",
    "                'text1_prompt': meta1['prompt_type'],\n",
    "                'text2_prompt': meta2['prompt_type'],\n",
    "                'text1_temp': meta1['temperature'],\n",
    "                'text2_temp': meta2['temperature'],\n",
    "                'text1_filename': meta1['filename'],\n",
    "                'text2_filename': meta2['filename'],\n",
    "                'precision': float(P.item()),  # Convert to native Python float\n",
    "                'recall': float(R.item()),     # Convert to native Python float\n",
    "                'f1_score': float(F1.item()),  # Convert to native Python float\n",
    "                'same_prompt': meta1['prompt_type'] == meta2['prompt_type'],\n",
    "                'same_temp': meta1['temperature'] == meta2['temperature'],\n",
    "                'temp_diff': abs(meta1['temperature'] - meta2['temperature'])\n",
    "            }\n",
    "            \n",
    "            bertscore_results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(bertscore_results)\n",
    "\n",
    "def analyze_semantic_coherence_enhanced(bertscore_df):\n",
    "    \"\"\"Enhanced semantic coherence analysis with multiple perspectives\"\"\"\n",
    "    print(\"Analyzing semantic coherence patterns...\")\n",
    "    \n",
    "    coherence_results = {}\n",
    "    \n",
    "    # 1. Coherence within same prompt type\n",
    "    same_prompt_coherence = bertscore_df[\n",
    "        bertscore_df['text1_prompt'] == bertscore_df['text2_prompt']\n",
    "    ].groupby(['text1_prompt'])['f1_score'].agg(['mean', 'std', 'count', 'median', 'min', 'max'])\n",
    "    \n",
    "    coherence_results['same_prompt'] = same_prompt_coherence\n",
    "    \n",
    "    # 2. Coherence between different prompt types\n",
    "    cross_prompt_coherence = bertscore_df[\n",
    "        bertscore_df['text1_prompt'] != bertscore_df['text2_prompt']\n",
    "    ]['f1_score'].agg(['mean', 'std', 'count', 'median', 'min', 'max'])\n",
    "    \n",
    "    coherence_results['cross_prompt'] = cross_prompt_coherence\n",
    "    \n",
    "    # 3. Enhanced temperature-based coherence analysis\n",
    "    temp_coherence = []\n",
    "    \n",
    "    # Same temperature pairs\n",
    "    for temp in TEMPERATURES:\n",
    "        temp_data = bertscore_df[\n",
    "            (bertscore_df['text1_temp'] == temp) & \n",
    "            (bertscore_df['text2_temp'] == temp)\n",
    "        ]\n",
    "        if len(temp_data) >= ANALYSIS_CONFIG['temperature_analysis']['min_pairs_per_temp']:\n",
    "            temp_coherence.append({\n",
    "                'temperature': float(temp),  # Convert to native Python float\n",
    "                'analysis_type': 'same_temperature',\n",
    "                'mean_f1': float(temp_data['f1_score'].mean()),\n",
    "                'std_f1': float(temp_data['f1_score'].std()),\n",
    "                'median_f1': float(temp_data['f1_score'].median()),\n",
    "                'count': int(len(temp_data)),  # Convert to native Python int\n",
    "                'q25': float(temp_data['f1_score'].quantile(0.25)),\n",
    "                'q75': float(temp_data['f1_score'].quantile(0.75))\n",
    "            })\n",
    "    \n",
    "    # Temperature difference analysis\n",
    "    temp_diff_bins = pd.cut(bertscore_df['temp_diff'], \n",
    "                           bins=ANALYSIS_CONFIG['temperature_analysis']['temperature_bins'],\n",
    "                           include_lowest=True)\n",
    "    \n",
    "    temp_diff_analysis = bertscore_df.groupby(temp_diff_bins)['f1_score'].agg([\n",
    "        'mean', 'std', 'count', 'median'\n",
    "    ]).reset_index()\n",
    "    temp_diff_analysis['temp_diff_range'] = temp_diff_analysis['temp_diff'].astype(str)\n",
    "    \n",
    "    coherence_results['temperature'] = pd.DataFrame(temp_coherence)\n",
    "    coherence_results['temperature_difference'] = temp_diff_analysis\n",
    "    \n",
    "    # 4. Correlation analysis between temperature and semantic similarity\n",
    "    correlation_results = {}\n",
    "    for method in ANALYSIS_CONFIG['temperature_analysis']['correlation_methods']:\n",
    "        if method == 'pearson':\n",
    "            corr_coeff, p_value = stats.pearsonr(bertscore_df['temp_diff'], bertscore_df['f1_score'])\n",
    "        elif method == 'spearman':\n",
    "            corr_coeff, p_value = stats.spearmanr(bertscore_df['temp_diff'], bertscore_df['f1_score'])\n",
    "        elif method == 'kendall':\n",
    "            corr_coeff, p_value = stats.kendalltau(bertscore_df['temp_diff'], bertscore_df['f1_score'])\n",
    "        \n",
    "        correlation_results[method] = {\n",
    "            'correlation': float(corr_coeff),  # Convert to native Python float\n",
    "            'p_value': float(p_value),         # Convert to native Python float\n",
    "            'significant': bool(p_value < ANALYSIS_CONFIG['alpha'])  # Convert to native Python bool\n",
    "        }\n",
    "    \n",
    "    coherence_results['temperature_correlations'] = correlation_results\n",
    "    \n",
    "    return coherence_results\n",
    "\n",
    "def perform_topic_modeling(texts, metadata):\n",
    "    \"\"\"Perform enhanced topic modeling analysis\"\"\"\n",
    "    print(\"Performing topic modeling analysis...\")\n",
    "    \n",
    "    # Vectorize texts with enhanced parameters\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        lowercase=True,\n",
    "        strip_accents='unicode'\n",
    "    )\n",
    "    \n",
    "    text_vectors = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Apply LDA with enhanced parameters\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=ANALYSIS_CONFIG['n_topics'],\n",
    "        random_state=42,\n",
    "        max_iter=100,\n",
    "        learning_method='batch',\n",
    "        evaluate_every=10,\n",
    "        perp_tol=0.1\n",
    "    )\n",
    "    \n",
    "    doc_topics = lda.fit_transform(text_vectors)\n",
    "    \n",
    "    # Create enhanced topic modeling results\n",
    "    topic_results = []\n",
    "    for i, doc_topic in enumerate(doc_topics):\n",
    "        dominant_topic = np.argmax(doc_topic)\n",
    "        topic_entropy = -np.sum(doc_topic * np.log(doc_topic + 1e-10))  # Topic entropy\n",
    "        \n",
    "        topic_results.append({\n",
    "            'text_idx': int(i),  # Convert to native Python int\n",
    "            'prompt_type': metadata[i]['prompt_type'],\n",
    "            'temperature': float(metadata[i]['temperature']),  # Convert to native Python float\n",
    "            'filename': metadata[i]['filename'],\n",
    "            'dominant_topic': int(dominant_topic),  # Convert to native Python int\n",
    "            'topic_probability': float(doc_topic[dominant_topic]),  # Convert to native Python float\n",
    "            'topic_entropy': float(topic_entropy),  # Convert to native Python float\n",
    "            'topic_distribution': [float(x) for x in doc_topic.tolist()]  # Convert all to native Python floats\n",
    "        })\n",
    "    \n",
    "    # Get enhanced topic words with weights\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_word_indices = topic.argsort()[-10:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_word_indices]\n",
    "        word_weights = topic[top_word_indices].tolist()\n",
    "        \n",
    "        topic_words.append({\n",
    "            'topic_id': int(topic_idx),  # Convert to native Python int\n",
    "            'top_words': top_words,\n",
    "            'word_weights': [float(w) for w in word_weights],  # Convert to native Python floats\n",
    "            'total_weight': float(np.sum(word_weights))  # Convert to native Python float\n",
    "        })\n",
    "    \n",
    "    # Topic distribution analysis by prompt and temperature\n",
    "    topic_df = pd.DataFrame(topic_results)\n",
    "    topic_distribution = topic_df.groupby(['prompt_type', 'temperature', 'dominant_topic']).size().reset_index(name='count')\n",
    "    topic_diversity = topic_df.groupby(['prompt_type', 'temperature'])['topic_entropy'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    return topic_df, topic_words, topic_distribution, topic_diversity\n",
    "\n",
    "def analyze_network_at_multiple_thresholds(bertscore_df, thresholds):\n",
    "    \"\"\"Analyze semantic similarity networks at multiple thresholds\"\"\"\n",
    "    print(\"Analyzing networks at multiple similarity thresholds...\")\n",
    "    \n",
    "    threshold_results = []\n",
    "    detailed_networks = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        network, metrics = create_semantic_similarity_network(bertscore_df, threshold)\n",
    "        \n",
    "        # Enhanced metrics calculation with type conversion\n",
    "        enhanced_metrics = {}\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (np.integer, np.floating)):\n",
    "                enhanced_metrics[key] = float(value) if isinstance(value, np.floating) else int(value)\n",
    "            else:\n",
    "                enhanced_metrics[key] = value\n",
    "        \n",
    "        if metrics['n_edges'] > 0:\n",
    "            # Calculate additional network properties\n",
    "            try:\n",
    "                degrees = [d for n, d in network.degree()]\n",
    "                enhanced_metrics['avg_degree'] = float(np.mean(degrees))\n",
    "                enhanced_metrics['max_degree'] = int(max(degrees))\n",
    "                \n",
    "                # Small world properties\n",
    "                if nx.is_connected(network):\n",
    "                    enhanced_metrics['diameter'] = int(nx.diameter(network))\n",
    "                    enhanced_metrics['radius'] = int(nx.radius(network))\n",
    "                    enhanced_metrics['average_shortest_path'] = float(nx.average_shortest_path_length(network))\n",
    "                \n",
    "                # Community detection (if available)\n",
    "                try:\n",
    "                    import community as community_louvain\n",
    "                    partition = community_louvain.best_partition(network)\n",
    "                    enhanced_metrics['n_communities'] = int(len(set(partition.values())))\n",
    "                    enhanced_metrics['modularity'] = float(community_louvain.modularity(partition, network))\n",
    "                except ImportError:\n",
    "                    enhanced_metrics['n_communities'] = None\n",
    "                    enhanced_metrics['modularity'] = None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating enhanced metrics for threshold {threshold}: {e}\")\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': float(threshold),  # Convert to native Python float\n",
    "            **enhanced_metrics\n",
    "        })\n",
    "        \n",
    "        detailed_networks[threshold] = network\n",
    "        \n",
    "        print(f\"Threshold {threshold}: {metrics['n_edges']} edges, density {metrics['density']:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(threshold_results), detailed_networks\n",
    "\n",
    "def create_semantic_similarity_network(bertscore_df, threshold=0.7):\n",
    "    \"\"\"Create enhanced semantic similarity network based on BERTScore\"\"\"\n",
    "    \n",
    "    # Create network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes (text indices) with metadata\n",
    "    unique_texts = set(bertscore_df['text1_idx'].unique()) | set(bertscore_df['text2_idx'].unique())\n",
    "    G.add_nodes_from(unique_texts)\n",
    "    \n",
    "    # Add edges based on similarity threshold with enhanced attributes\n",
    "    for _, row in bertscore_df.iterrows():\n",
    "        if row['f1_score'] >= threshold:\n",
    "            G.add_edge(\n",
    "                row['text1_idx'], \n",
    "                row['text2_idx'], \n",
    "                weight=float(row['f1_score']),\n",
    "                precision=float(row['precision']),\n",
    "                recall=float(row['recall']),\n",
    "                temp_diff=float(row['temp_diff']),\n",
    "                same_prompt=bool(row['same_prompt']),\n",
    "                same_temp=bool(row['same_temp'])\n",
    "            )\n",
    "    \n",
    "    # Compute enhanced network metrics\n",
    "    network_metrics = {\n",
    "        'n_nodes': int(G.number_of_nodes()),\n",
    "        'n_edges': int(G.number_of_edges()),\n",
    "        'density': float(nx.density(G) if G.number_of_nodes() > 1 else 0),\n",
    "        'avg_clustering': float(nx.average_clustering(G)),\n",
    "        'n_components': int(nx.number_connected_components(G))\n",
    "    }\n",
    "    \n",
    "    if G.number_of_edges() > 0:\n",
    "        try:\n",
    "            largest_cc = max(nx.connected_components(G), key=len)\n",
    "            largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "            \n",
    "            if len(largest_cc) > 1:\n",
    "                network_metrics['largest_component_size'] = int(len(largest_cc))\n",
    "                network_metrics['avg_path_length_largest_cc'] = float(nx.average_shortest_path_length(largest_cc_subgraph))\n",
    "            else:\n",
    "                network_metrics['largest_component_size'] = 1\n",
    "                network_metrics['avg_path_length_largest_cc'] = 0.0\n",
    "        except:\n",
    "            network_metrics['largest_component_size'] = 0\n",
    "            network_metrics['avg_path_length_largest_cc'] = None\n",
    "    else:\n",
    "        network_metrics['largest_component_size'] = 0\n",
    "        network_metrics['avg_path_length_largest_cc'] = None\n",
    "    \n",
    "    return G, network_metrics\n",
    "\n",
    "def convert_for_json(obj):\n",
    "    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, pd.Series):\n",
    "        return obj.to_dict()\n",
    "    elif isinstance(obj, pd.DataFrame):\n",
    "        return {k: convert_for_json(v) for k, v in obj.to_dict().items()}\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def create_enhanced_visualizations(bertscore_df, coherence_results, threshold_results):\n",
    "    \"\"\"Create comprehensive visualizations for semantic analysis\"\"\"\n",
    "    print(\"Creating enhanced visualizations...\")\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')  # Use default style instead of seaborn\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    \n",
    "    # 1. BERTScore distribution by prompt type\n",
    "    ax1 = plt.subplot(4, 3, 1)\n",
    "    sns.boxplot(data=bertscore_df, x='text1_prompt', y='f1_score', ax=ax1)\n",
    "    ax1.set_title('BERTScore F1 Distribution by Prompt Type')\n",
    "    ax1.set_xlabel('Prompt Type')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    \n",
    "    # 2. Temperature difference vs semantic similarity\n",
    "    ax2 = plt.subplot(4, 3, 2)\n",
    "    scatter = ax2.scatter(bertscore_df['temp_diff'], bertscore_df['f1_score'], \n",
    "                         c=bertscore_df['same_prompt'].astype(int), \n",
    "                         alpha=0.6, cmap='viridis')\n",
    "    ax2.set_xlabel('Temperature Difference')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.set_title('Temperature Difference vs Semantic Similarity')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Same Prompt Type')\n",
    "    \n",
    "    # 3. Network metrics across thresholds\n",
    "    ax3 = plt.subplot(4, 3, 3)\n",
    "    ax3.plot(threshold_results['threshold'], threshold_results['n_edges'], 'o-', label='Edges')\n",
    "    ax3.set_xlabel('Similarity Threshold')\n",
    "    ax3.set_ylabel('Number of Edges')\n",
    "    ax3.set_title('Network Edges vs Threshold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Network density across thresholds\n",
    "    ax4 = plt.subplot(4, 3, 4)\n",
    "    ax4.plot(threshold_results['threshold'], threshold_results['density'], 's-', color='red', label='Density')\n",
    "    ax4.set_xlabel('Similarity Threshold')\n",
    "    ax4.set_ylabel('Network Density')\n",
    "    ax4.set_title('Network Density vs Threshold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Same vs cross-prompt coherence comparison\n",
    "    ax5 = plt.subplot(4, 3, 5)\n",
    "    if 'same_prompt' in coherence_results and not coherence_results['same_prompt'].empty:\n",
    "        prompt_means = coherence_results['same_prompt']['mean']\n",
    "        ax5.bar(range(len(prompt_means)), prompt_means.values, \n",
    "               yerr=coherence_results['same_prompt']['std'].values,\n",
    "               alpha=0.7, capsize=5)\n",
    "        ax5.set_xticks(range(len(prompt_means)))\n",
    "        ax5.set_xticklabels(prompt_means.index)\n",
    "        ax5.set_ylabel('Mean F1 Score')\n",
    "        ax5.set_title('Within-Prompt Type Coherence')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Temperature correlation analysis\n",
    "    ax6 = plt.subplot(4, 3, 6)\n",
    "    if 'temperature_correlations' in coherence_results:\n",
    "        methods = list(coherence_results['temperature_correlations'].keys())\n",
    "        correlations = [coherence_results['temperature_correlations'][m]['correlation'] for m in methods]\n",
    "        p_values = [coherence_results['temperature_correlations'][m]['p_value'] for m in methods]\n",
    "        \n",
    "        bars = ax6.bar(methods, correlations, alpha=0.7)\n",
    "        # Color bars based on significance\n",
    "        for i, (bar, p_val) in enumerate(zip(bars, p_values)):\n",
    "            if p_val < 0.05:\n",
    "                bar.set_color('red')\n",
    "            else:\n",
    "                bar.set_color('gray')\n",
    "        \n",
    "        ax6.set_ylabel('Correlation Coefficient')\n",
    "        ax6.set_title('Temperature-Similarity Correlations')\n",
    "        ax6.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. BERTScore component analysis\n",
    "    ax7 = plt.subplot(4, 3, 7)\n",
    "    ax7.scatter(bertscore_df['precision'], bertscore_df['recall'], \n",
    "               c=bertscore_df['f1_score'], alpha=0.6, cmap='plasma')\n",
    "    ax7.set_xlabel('Precision')\n",
    "    ax7.set_ylabel('Recall')\n",
    "    ax7.set_title('BERTScore Components Analysis')\n",
    "    \n",
    "    # 8. Network components analysis\n",
    "    ax8 = plt.subplot(4, 3, 8)\n",
    "    ax8.plot(threshold_results['threshold'], threshold_results['n_components'], 'd-', color='green')\n",
    "    ax8.set_xlabel('Similarity Threshold')\n",
    "    ax8.set_ylabel('Number of Components')\n",
    "    ax8.set_title('Network Fragmentation vs Threshold')\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Temperature effect heatmap\n",
    "    ax9 = plt.subplot(4, 3, 9)\n",
    "    if 'temperature_difference' in coherence_results and not coherence_results['temperature_difference'].empty:\n",
    "        temp_data = coherence_results['temperature_difference']\n",
    "        if len(temp_data) > 1:\n",
    "            ax9.bar(range(len(temp_data)), temp_data['mean'], \n",
    "                   yerr=temp_data['std'], alpha=0.7, capsize=3)\n",
    "            ax9.set_xticks(range(len(temp_data)))\n",
    "            ax9.set_xticklabels([str(x) for x in temp_data['temp_diff_range']], rotation=45)\n",
    "            ax9.set_ylabel('Mean F1 Score')\n",
    "            ax9.set_title('Similarity by Temperature Difference')\n",
    "            ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 10. Distribution of F1 scores\n",
    "    ax10 = plt.subplot(4, 3, 10)\n",
    "    ax10.hist(bertscore_df['f1_score'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax10.axvline(bertscore_df['f1_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    ax10.axvline(bertscore_df['f1_score'].median(), color='green', linestyle='--', label='Median')\n",
    "    ax10.set_xlabel('F1 Score')\n",
    "    ax10.set_ylabel('Frequency')\n",
    "    ax10.set_title('F1 Score Distribution')\n",
    "    ax10.legend()\n",
    "    ax10.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 11. Clustering coefficient analysis\n",
    "    ax11 = plt.subplot(4, 3, 11)\n",
    "    if 'avg_clustering' in threshold_results.columns:\n",
    "        ax11.plot(threshold_results['threshold'], threshold_results['avg_clustering'], 'o-', color='purple')\n",
    "        ax11.set_xlabel('Similarity Threshold')\n",
    "        ax11.set_ylabel('Average Clustering Coefficient')\n",
    "        ax11.set_title('Network Clustering vs Threshold')\n",
    "        ax11.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 12. Summary statistics table\n",
    "    ax12 = plt.subplot(4, 3, 12)\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    summary_text = f\"\"\"\n",
    "    SEMANTIC ANALYSIS SUMMARY\n",
    "    \n",
    "    Total text pairs analyzed: {len(bertscore_df)}\n",
    "    Mean F1 Score: {bertscore_df['f1_score'].mean():.3f} ± {bertscore_df['f1_score'].std():.3f}\n",
    "    \n",
    "    Within-prompt coherence:\n",
    "    Complex: {coherence_results['same_prompt'].loc['complex', 'mean']:.3f} ± {coherence_results['same_prompt'].loc['complex', 'std']:.3f}\n",
    "    Vague: {coherence_results['same_prompt'].loc['vague', 'mean']:.3f} ± {coherence_results['same_prompt'].loc['vague', 'std']:.3f}\n",
    "    \n",
    "    Cross-prompt coherence: {coherence_results['cross_prompt']['mean']:.3f} ± {coherence_results['cross_prompt']['std']:.3f}\n",
    "    \n",
    "    Optimal threshold: {threshold_results[threshold_results['n_edges'] > 0]['threshold'].min():.1f}\n",
    "    Max network density: {threshold_results['density'].max():.3f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax12.text(0.1, 0.9, summary_text, transform=ax12.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the comprehensive visualization\n",
    "    viz_path = os.path.join(semantic_results_dir, 'comprehensive_semantic_analysis.png')\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Enhanced visualizations saved to: {viz_path}\")\n",
    "    return viz_path\n",
    "\n",
    "# Main enhanced semantic analysis execution\n",
    "print(\"Loading texts for semantic analysis...\")\n",
    "texts, metadata = load_texts_for_semantic_analysis()\n",
    "\n",
    "if texts:\n",
    "    print(f\"Loaded {len(texts)} texts for semantic analysis\")\n",
    "    \n",
    "    # Compute BERTScore similarities\n",
    "    bertscore_df = compute_bertscore_similarities(texts, metadata)\n",
    "    \n",
    "    if not bertscore_df.empty:\n",
    "        print(f\"Computed {len(bertscore_df)} BERTScore similarity pairs\")\n",
    "        \n",
    "        # Save BERTScore results\n",
    "        bertscore_file = os.path.join(semantic_results_dir, 'bertscore_similarities.csv')\n",
    "        bertscore_df.to_csv(bertscore_file, index=False)\n",
    "        print(f\"BERTScore results saved to {bertscore_file}\")\n",
    "        \n",
    "        # Enhanced semantic coherence analysis\n",
    "        coherence_results = analyze_semantic_coherence_enhanced(bertscore_df)\n",
    "        \n",
    "        print(\"\\nENHANCED SEMANTIC COHERENCE ANALYSIS:\")\n",
    "        print(\"Same prompt type coherence:\")\n",
    "        print(coherence_results['same_prompt'])\n",
    "        \n",
    "        print(f\"\\nCross prompt type coherence:\")\n",
    "        cross_prompt = coherence_results['cross_prompt']\n",
    "        print(f\"  Mean F1: {cross_prompt['mean']:.4f} ± {cross_prompt['std']:.4f}\")\n",
    "        print(f\"  Median F1: {cross_prompt['median']:.4f}\")\n",
    "        print(f\"  Range: [{cross_prompt['min']:.4f}, {cross_prompt['max']:.4f}]\")\n",
    "        \n",
    "        if not coherence_results['temperature'].empty:\n",
    "            print(\"\\nTemperature-based coherence (same temperature pairs):\")\n",
    "            print(coherence_results['temperature'][['temperature', 'mean_f1', 'std_f1', 'count']])\n",
    "        \n",
    "        print(\"\\nTemperature correlation analysis:\")\n",
    "        for method, result in coherence_results['temperature_correlations'].items():\n",
    "            significance = \"***\" if result['significant'] else \"n.s.\"\n",
    "            print(f\"  {method}: r = {result['correlation']:.4f}, p = {result['p_value']:.4f} {significance}\")\n",
    "        \n",
    "        # Enhanced topic modeling\n",
    "        topic_results, topic_words, topic_distribution, topic_diversity = perform_topic_modeling(texts, metadata)\n",
    "        \n",
    "        if not topic_results.empty:\n",
    "            print(f\"\\nENHANCED TOPIC MODELING RESULTS:\")\n",
    "            print(f\"Identified {ANALYSIS_CONFIG['n_topics']} topics\")\n",
    "            \n",
    "            # Save enhanced topic results\n",
    "            topic_file = os.path.join(semantic_results_dir, 'enhanced_topic_modeling_results.csv')\n",
    "            topic_results.to_csv(topic_file, index=False)\n",
    "            \n",
    "            topic_dist_file = os.path.join(semantic_results_dir, 'topic_distribution_analysis.csv')\n",
    "            topic_distribution.to_csv(topic_dist_file, index=False)\n",
    "            \n",
    "            topic_div_file = os.path.join(semantic_results_dir, 'topic_diversity_analysis.csv')\n",
    "            topic_diversity.to_csv(topic_div_file, index=False)\n",
    "            \n",
    "            # Display enhanced topic summary\n",
    "            print(\"Topic distribution by prompt type and temperature:\")\n",
    "            print(topic_distribution.head(10))\n",
    "            \n",
    "            print(\"\\nTopic diversity (entropy) by conditions:\")\n",
    "            print(topic_diversity)\n",
    "        \n",
    "        # Multi-threshold network analysis\n",
    "        threshold_results, detailed_networks = analyze_network_at_multiple_thresholds(\n",
    "            bertscore_df, \n",
    "            ANALYSIS_CONFIG['similarity_thresholds']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nMULTI-THRESHOLD NETWORK ANALYSIS:\")\n",
    "        print(threshold_results[['threshold', 'n_edges', 'density', 'n_components', 'avg_clustering']])\n",
    "        \n",
    "        # Find optimal threshold (first threshold with edges)\n",
    "        thresholds_with_edges = threshold_results[threshold_results['n_edges'] > 0]\n",
    "        if not thresholds_with_edges.empty:\n",
    "            optimal_threshold = thresholds_with_edges['threshold'].max()  # Use highest threshold with edges\n",
    "            print(f\"\\nOptimal threshold for analysis: {optimal_threshold}\")\n",
    "            \n",
    "            # Create network with optimal threshold\n",
    "            optimal_network, optimal_metrics = create_semantic_similarity_network(\n",
    "                bertscore_df, \n",
    "                threshold=optimal_threshold\n",
    "            )\n",
    "            \n",
    "            print(f\"OPTIMAL SEMANTIC SIMILARITY NETWORK (threshold={optimal_threshold}):\")\n",
    "            for metric, value in optimal_metrics.items():\n",
    "                print(f\"  {metric}: {value}\")\n",
    "        else:\n",
    "            print(f\"\\nNo threshold produced edges. Consider lowering thresholds further.\")\n",
    "            optimal_threshold = ANALYSIS_CONFIG['similarity_threshold']\n",
    "        \n",
    "        # Save network analysis results\n",
    "        threshold_file = os.path.join(semantic_results_dir, 'multi_threshold_analysis.csv')\n",
    "        threshold_results.to_csv(threshold_file, index=False)\n",
    "        \n",
    "        # Save network metrics with proper JSON serialization\n",
    "        network_file = os.path.join(semantic_results_dir, 'enhanced_semantic_network_metrics.json')\n",
    "        import json\n",
    "        \n",
    "        # Convert all data to JSON-serializable format\n",
    "        network_data = {\n",
    "            'analysis_parameters': convert_for_json(ANALYSIS_CONFIG),\n",
    "            'threshold_analysis': convert_for_json(threshold_results.to_dict('records')),\n",
    "            'coherence_results': {\n",
    "                'same_prompt': convert_for_json(coherence_results['same_prompt'].to_dict()),\n",
    "                'cross_prompt': convert_for_json(coherence_results['cross_prompt'].to_dict()),\n",
    "                'temperature_correlations': convert_for_json(coherence_results['temperature_correlations'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(network_file, 'w') as f:\n",
    "            json.dump(network_data, f, indent=2)\n",
    "        \n",
    "        # Create enhanced visualizations\n",
    "        viz_path = create_enhanced_visualizations(bertscore_df, coherence_results, threshold_results)\n",
    "        \n",
    "        print(f\"\\nENHANCED SEMANTIC ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"Results saved in: {semantic_results_dir}\")\n",
    "        print(f\"Key findings:\")\n",
    "        print(f\"  - Complex prompts show {(coherence_results['same_prompt'].loc['complex', 'mean'] / coherence_results['same_prompt'].loc['vague', 'mean'] - 1) * 100:.1f}% higher coherence\")\n",
    "        print(f\"  - Temperature effects: {list(coherence_results['temperature_correlations'].keys())} correlations computed\")\n",
    "        print(f\"  - Network analysis: {len(ANALYSIS_CONFIG['similarity_thresholds'])} thresholds analyzed\")\n",
    "        print(f\"  - Topic modeling: {ANALYSIS_CONFIG['n_topics']} topics identified with diversity analysis\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No BERTScore similarities computed\")\n",
    "else:\n",
    "    print(\"No texts found for semantic analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12821ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing statistical analyzer...\n",
      "Loaded network metrics: 14 networks\n",
      "Starting comprehensive statistical analysis...\n",
      "Starting comprehensive statistical analysis...\n",
      "Running normality tests...\n",
      "Running normality tests...\n",
      "Running pairwise comparisons...\n",
      "Running pairwise comparisons...\n",
      "Running ANOVA analysis...\n",
      "Running ANOVA analysis...\n",
      "Calculating effect sizes...\n",
      "Running effect size calculations...\n",
      "Running PCA analysis...\n",
      "Running PCA analysis...\n",
      "Running clustering analysis...\n",
      "Running clustering analysis...\n",
      "Running regression analysis...\n",
      "Running regression analysis...\n",
      "Advanced analysis completed: 84 results generated\n",
      "Statistical analysis completed successfully!\n",
      "Total results: 84\n",
      "Results saved to: results/advanced_statistical_analysis.csv\n",
      "Analysis summary:\n",
      "  - normality_test: 20 results\n",
      "  - correlation: 20 results\n",
      "  - group_comparison: 10 results\n",
      "  - anova: 10 results\n",
      "  - effect_size: 10 results\n",
      "  - pca: 10 results\n",
      "  - clustering: 4 results\n",
      "\n",
      "============================================================\n",
      "ENHANCED BASIC STATISTICS\n",
      "============================================================\n",
      "\n",
      "Descriptive Statistics by Prompt Type:\n",
      "            density                                \n",
      "              count    mean     std     min     max\n",
      "prompt_type                                        \n",
      "complex           7  0.0205  0.0021  0.0181  0.0245\n",
      "vague             7  0.0164  0.0010  0.0150  0.0174\n",
      "\n",
      "Temperature Effect Analysis:\n",
      "               min  max    mean     std\n",
      "prompt_type                            \n",
      "complex      0.001  1.5  0.7501  0.5398\n",
      "vague        0.001  1.5  0.7501  0.5398\n",
      "\n",
      "Correlation Matrix:\n",
      "             density  temperature\n",
      "density        1.000       -0.506\n",
      "temperature   -0.506        1.000\n",
      "\n",
      "Statistical analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize statistical analyzer\n",
    "print(\"Initializing statistical analyzer...\")\n",
    "stat_analyzer = StatisticalAnalyzer(alpha=ANALYSIS_CONFIG['alpha'])\n",
    "\n",
    "# Check if we have network metrics data\n",
    "results_file = os.path.join(DIRS['results'], 'network_metrics.csv')\n",
    "\n",
    "if not os.path.exists(results_file):\n",
    "    print(\"Network metrics file not found. Please run network analysis first.\")\n",
    "    df = pd.DataFrame()\n",
    "else:\n",
    "    # Load network metrics\n",
    "    df = pd.read_csv(results_file)\n",
    "    print(f\"Loaded network metrics: {len(df)} networks\")\n",
    "    \n",
    "    # Check if advanced analysis has already been performed\n",
    "    advanced_results_file = os.path.join(DIRS['results'], 'advanced_statistical_analysis.csv')\n",
    "    \n",
    "    if os.path.exists(advanced_results_file):\n",
    "        print(\"Advanced statistical analysis already completed!\")\n",
    "        advanced_df = pd.read_csv(advanced_results_file)\n",
    "        print(f\"Analysis results: {len(advanced_df)} records\")\n",
    "    else:\n",
    "        print(\"Starting comprehensive statistical analysis...\")\n",
    "        \n",
    "        # Run advanced analysis using the fixed StatisticalAnalyzer class\n",
    "        advanced_df = stat_analyzer.run_advanced_analysis(df)\n",
    "        \n",
    "        if not advanced_df.empty:\n",
    "            # Save results\n",
    "            advanced_df.to_csv(advanced_results_file, index=False)\n",
    "            print(f\"Statistical analysis completed successfully!\")\n",
    "            print(f\"Total results: {len(advanced_df)}\")\n",
    "            print(f\"Results saved to: {advanced_results_file}\")\n",
    "            \n",
    "            # Display summary of analysis types\n",
    "            analysis_summary = advanced_df['analysis_type'].value_counts()\n",
    "            print(f\"Analysis summary:\")\n",
    "            for analysis_type, count in analysis_summary.items():\n",
    "                print(f\"  - {analysis_type}: {count} results\")\n",
    "        else:\n",
    "            print(\"No analysis results generated.\")\n",
    "    \n",
    "    # Display enhanced basic statistics\n",
    "    if not df.empty:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENHANCED BASIC STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        key_metrics = ['num_nodes', 'num_edges', 'density', 'clustering_coefficient']\n",
    "        available_metrics = [col for col in key_metrics if col in df.columns]\n",
    "        \n",
    "        if available_metrics:\n",
    "            print(\"\\nDescriptive Statistics by Prompt Type:\")\n",
    "            summary_stats = df.groupby('prompt_type')[available_metrics].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "            print(summary_stats.round(4))\n",
    "            \n",
    "            print(\"\\nTemperature Effect Analysis:\")\n",
    "            temp_stats = df.groupby('prompt_type')['temperature'].agg(['min', 'max', 'mean', 'std'])\n",
    "            print(temp_stats.round(4))\n",
    "            \n",
    "            # Clean correlation matrix\n",
    "            corr_data = df[available_metrics + ['temperature']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if not corr_data.empty:\n",
    "                print(\"\\nCorrelation Matrix:\")\n",
    "                corr_matrix = corr_data.corr()\n",
    "                print(corr_matrix.round(3))\n",
    "            \n",
    "        print(f\"\\nStatistical analysis completed successfully!\")\n",
    "    else:\n",
    "        print(\"No data available for statistical analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c661e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOTSTRAP ANALYSIS FOR ROBUST STATISTICAL INFERENCE\n",
      "Starting comprehensive bootstrap analysis...\n",
      "Performing bootstrap analysis with 1000 samples...\n",
      "Bootstrap confidence intervals computed for 20 metric-prompt combinations\n",
      "Bootstrap results saved to bootstrap_results/network_metrics_bootstrap.csv\n",
      "\n",
      "BOOTSTRAP CONFIDENCE INTERVALS (95%):\n",
      "  complex - nodes: 1347.2301 [1192.1700, 1482.6850]\n",
      "  complex - edges: 18547.9497 [15623.3714, 21091.8607]\n",
      "  complex - density: 0.0205 [0.0193, 0.0221]\n",
      "  complex - clustering: 0.5037 [0.5017, 0.5056]\n",
      "  complex - path_len: 2.5416 [2.5343, 2.5505]\n",
      "  complex - avg_deg: 27.2637 [25.8367, 28.4866]\n",
      "  complex - max_deg: 456.7406 [401.6611, 503.7214]\n",
      "  complex - std_deg: 41.2590 [38.0863, 43.9759]\n",
      "  complex - diameter: 5.2862 [5.0000, 5.6667]\n",
      "  complex - transitivity: 0.1985 [0.1947, 0.2026]\n",
      "Analyzing temperature effects with bootstrap...\n",
      "\n",
      "TEMPERATURE EFFECT CONFIDENCE INTERVALS:\n",
      "  nodes: r = 0.7779 [0.5587, 0.9131]\n",
      "  edges: r = 0.8737 [0.7479, 0.9540]\n",
      "  density: r = -0.4842 [-0.8432, 0.0548]\n",
      "  clustering: r = -0.0161 [-0.5924, 0.5751]\n",
      "  path_len: r = 0.0471 [-0.6121, 0.6622]\n",
      "  avg_deg: r = 0.8783 [0.7963, 0.9562]\n",
      "  max_deg: r = 0.5175 [0.0542, 0.8167]\n",
      "  std_deg: r = 0.9144 [0.8524, 0.9762]\n",
      "  diameter: r = -0.0374 [nan, nan]\n",
      "  transitivity: r = -0.1437 [-0.6898, 0.5017]\n",
      "Computing effect sizes...\n",
      "\n",
      "EFFECT SIZES:\n",
      "  nodes: Cohen's d = -1.3113 (large)\n",
      "  edges: Cohen's d = -0.7840 (medium)\n",
      "  density: Cohen's d = 2.5357 (large)\n",
      "  clustering: Cohen's d = 8.7549 (large)\n",
      "  path_len: Cohen's d = -3.3478 (large)\n",
      "  avg_deg: Cohen's d = 0.2823 (small)\n",
      "  max_deg: Cohen's d = -2.4215 (large)\n",
      "  std_deg: Cohen's d = 0.1736 (negligible)\n",
      "  diameter: Cohen's d = -1.0894 (large)\n",
      "  transitivity: Cohen's d = 6.1108 (large)\n",
      "Bootstrap analysis of semantic coherence...\n",
      "\n",
      "SEMANTIC COHERENCE BOOTSTRAP:\n",
      "                               mean       std\n",
      "analysis_type prompt_type                    \n",
      "cross_prompt  all          0.825326  0.000774\n",
      "within_prompt complex      0.864785  0.002576\n",
      "              vague        0.866209  0.002328\n",
      "\n",
      "Bootstrap analysis completed!\n",
      "Results saved in: bootstrap_results\n"
     ]
    }
   ],
   "source": [
    "## 7.2. Bootstrap Analysis for Robust Statistical Inference\n",
    "\n",
    "# This section implements bootstrap resampling to provide robust confidence intervals and statistical validation for all network metrics and semantic analysis results.\n",
    "\n",
    "# Bootstrap Analysis Implementation\n",
    "print(\"BOOTSTRAP ANALYSIS FOR ROBUST STATISTICAL INFERENCE\")\n",
    "\n",
    "def bootstrap_network_metrics(df, n_bootstrap=1000, ci_level=0.95):\n",
    "    \"\"\"\n",
    "    Perform bootstrap resampling on network metrics\n",
    "    \"\"\"\n",
    "    print(f\"Performing bootstrap analysis with {n_bootstrap} samples...\")\n",
    "    \n",
    "    bootstrap_results = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Remove non-metric columns\n",
    "    metric_cols = [col for col in numeric_cols if col not in ['temperature']]\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        bootstrap_sample = df.sample(n=len(df), replace=True, random_state=i)\n",
    "        \n",
    "        # Compute metrics for each group\n",
    "        for prompt_type in df['prompt_type'].unique():\n",
    "            prompt_data = bootstrap_sample[bootstrap_sample['prompt_type'] == prompt_type]\n",
    "            \n",
    "            for col in metric_cols:\n",
    "                if col in prompt_data.columns:\n",
    "                    bootstrap_results.append({\n",
    "                        'bootstrap_id': i,\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'metric': col,\n",
    "                        'value': prompt_data[col].mean(),\n",
    "                        'std': prompt_data[col].std(),\n",
    "                        'median': prompt_data[col].median()\n",
    "                    })\n",
    "    \n",
    "    bootstrap_df = pd.DataFrame(bootstrap_results)\n",
    "    \n",
    "    # Compute confidence intervals\n",
    "    alpha = 1 - ci_level\n",
    "    lower_percentile = (alpha/2) * 100\n",
    "    upper_percentile = (1 - alpha/2) * 100\n",
    "    \n",
    "    ci_results = []\n",
    "    for prompt_type in df['prompt_type'].unique():\n",
    "        for metric in metric_cols:\n",
    "            metric_data = bootstrap_df[\n",
    "                (bootstrap_df['prompt_type'] == prompt_type) & \n",
    "                (bootstrap_df['metric'] == metric)\n",
    "            ]['value']\n",
    "            \n",
    "            if len(metric_data) > 0:\n",
    "                ci_results.append({\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'metric': metric,\n",
    "                    'mean': metric_data.mean(),\n",
    "                    'std': metric_data.std(),\n",
    "                    'ci_lower': np.percentile(metric_data, lower_percentile),\n",
    "                    'ci_upper': np.percentile(metric_data, upper_percentile),\n",
    "                    'original_mean': df[df['prompt_type'] == prompt_type][metric].mean()\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(ci_results), bootstrap_df\n",
    "\n",
    "def bootstrap_temperature_effects(df, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Bootstrap analysis of temperature effects on network metrics\n",
    "    \"\"\"\n",
    "    print(\"Analyzing temperature effects with bootstrap...\")\n",
    "    \n",
    "    bootstrap_temp_results = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    metric_cols = [col for col in numeric_cols if col not in ['temperature']]\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        bootstrap_sample = df.sample(n=len(df), replace=True, random_state=i)\n",
    "        \n",
    "        for metric in metric_cols:\n",
    "            if metric in bootstrap_sample.columns:\n",
    "                # Compute correlation with temperature\n",
    "                temp_corr = bootstrap_sample['temperature'].corr(bootstrap_sample[metric])\n",
    "                \n",
    "                bootstrap_temp_results.append({\n",
    "                    'bootstrap_id': i,\n",
    "                    'metric': metric,\n",
    "                    'temperature_correlation': temp_corr\n",
    "                })\n",
    "    \n",
    "    temp_bootstrap_df = pd.DataFrame(bootstrap_temp_results)\n",
    "    \n",
    "    # Compute confidence intervals for correlations\n",
    "    temp_ci_results = []\n",
    "    for metric in metric_cols:\n",
    "        metric_corrs = temp_bootstrap_df[temp_bootstrap_df['metric'] == metric]['temperature_correlation']\n",
    "        \n",
    "        if len(metric_corrs) > 0:\n",
    "            temp_ci_results.append({\n",
    "                'metric': metric,\n",
    "                'mean_correlation': metric_corrs.mean(),\n",
    "                'std_correlation': metric_corrs.std(),\n",
    "                'ci_lower': np.percentile(metric_corrs, 2.5),\n",
    "                'ci_upper': np.percentile(metric_corrs, 97.5),\n",
    "                'original_correlation': df['temperature'].corr(df[metric])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(temp_ci_results)\n",
    "\n",
    "def bootstrap_semantic_coherence(bertscore_df, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Bootstrap analysis of semantic coherence metrics\n",
    "    \"\"\"\n",
    "    print(\"Bootstrap analysis of semantic coherence...\")\n",
    "    \n",
    "    bootstrap_coherence_results = []\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        bootstrap_sample = bertscore_df.sample(n=len(bertscore_df), replace=True, random_state=i)\n",
    "        \n",
    "        # Within-prompt coherence\n",
    "        same_prompt_data = bootstrap_sample[\n",
    "            bootstrap_sample['text1_prompt'] == bootstrap_sample['text2_prompt']\n",
    "        ]\n",
    "        \n",
    "        if len(same_prompt_data) > 0:\n",
    "            for prompt_type in same_prompt_data['text1_prompt'].unique():\n",
    "                prompt_data = same_prompt_data[same_prompt_data['text1_prompt'] == prompt_type]\n",
    "                \n",
    "                bootstrap_coherence_results.append({\n",
    "                    'bootstrap_id': i,\n",
    "                    'analysis_type': 'within_prompt',\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'mean_f1': prompt_data['f1_score'].mean(),\n",
    "                    'std_f1': prompt_data['f1_score'].std()\n",
    "                })\n",
    "        \n",
    "        # Cross-prompt coherence\n",
    "        cross_prompt_data = bootstrap_sample[\n",
    "            bootstrap_sample['text1_prompt'] != bootstrap_sample['text2_prompt']\n",
    "        ]\n",
    "        \n",
    "        if len(cross_prompt_data) > 0:\n",
    "            bootstrap_coherence_results.append({\n",
    "                'bootstrap_id': i,\n",
    "                'analysis_type': 'cross_prompt',\n",
    "                'prompt_type': 'all',\n",
    "                'mean_f1': cross_prompt_data['f1_score'].mean(),\n",
    "                'std_f1': cross_prompt_data['f1_score'].std()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(bootstrap_coherence_results)\n",
    "\n",
    "def compute_effect_sizes(df):\n",
    "    \"\"\"\n",
    "    Compute effect sizes for differences between prompt types\n",
    "    \"\"\"\n",
    "    print(\"Computing effect sizes...\")\n",
    "    \n",
    "    effect_sizes = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    metric_cols = [col for col in numeric_cols if col not in ['temperature']]\n",
    "    \n",
    "    if len(df['prompt_type'].unique()) >= 2:\n",
    "        prompt_types = df['prompt_type'].unique()\n",
    "        \n",
    "        for metric in metric_cols:\n",
    "            if metric in df.columns:\n",
    "                group1_data = df[df['prompt_type'] == prompt_types[0]][metric].dropna()\n",
    "                group2_data = df[df['prompt_type'] == prompt_types[1]][metric].dropna()\n",
    "                \n",
    "                if len(group1_data) > 0 and len(group2_data) > 0:\n",
    "                    # Cohen's d\n",
    "                    pooled_std = np.sqrt(((len(group1_data) - 1) * group1_data.var() + \n",
    "                                        (len(group2_data) - 1) * group2_data.var()) / \n",
    "                                       (len(group1_data) + len(group2_data) - 2))\n",
    "                    \n",
    "                    cohens_d = (group1_data.mean() - group2_data.mean()) / pooled_std\n",
    "                    \n",
    "                    # Hedges' g (bias-corrected)\n",
    "                    correction_factor = 1 - (3 / (4 * (len(group1_data) + len(group2_data) - 2) - 1))\n",
    "                    hedges_g = cohens_d * correction_factor\n",
    "                    \n",
    "                    effect_sizes.append({\n",
    "                        'metric': metric,\n",
    "                        'group1': prompt_types[0],\n",
    "                        'group2': prompt_types[1],\n",
    "                        'group1_mean': group1_data.mean(),\n",
    "                        'group2_mean': group2_data.mean(),\n",
    "                        'cohens_d': cohens_d,\n",
    "                        'hedges_g': hedges_g,\n",
    "                        'interpretation': interpret_effect_size(abs(cohens_d))\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(effect_sizes)\n",
    "\n",
    "def interpret_effect_size(effect_size):\n",
    "    \"\"\"\n",
    "    Interpret effect size magnitude\n",
    "    \"\"\"\n",
    "    if effect_size < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif effect_size < 0.5:\n",
    "        return \"small\"\n",
    "    elif effect_size < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "# Execute bootstrap analysis\n",
    "if 'df' in locals() and not df.empty:\n",
    "    print(\"Starting comprehensive bootstrap analysis...\")\n",
    "    \n",
    "    # Network metrics bootstrap\n",
    "    ci_results, bootstrap_df = bootstrap_network_metrics(\n",
    "        df, \n",
    "        n_bootstrap=ANALYSIS_CONFIG['n_bootstrap'],\n",
    "        ci_level=ANALYSIS_CONFIG['bootstrap_ci']\n",
    "    )\n",
    "    \n",
    "    if not ci_results.empty:\n",
    "        print(f\"Bootstrap confidence intervals computed for {len(ci_results)} metric-prompt combinations\")\n",
    "        \n",
    "        # Save bootstrap results\n",
    "        bootstrap_file = os.path.join(DIRS['bootstrap'], 'network_metrics_bootstrap.csv')\n",
    "        ci_results.to_csv(bootstrap_file, index=False)\n",
    "        print(f\"Bootstrap results saved to {bootstrap_file}\")\n",
    "        \n",
    "        # Display key results\n",
    "        print(\"\\nBOOTSTRAP CONFIDENCE INTERVALS (95%):\")\n",
    "        for _, row in ci_results.head(10).iterrows():\n",
    "            print(f\"  {row['prompt_type']} - {row['metric']}: \"\n",
    "                  f\"{row['mean']:.4f} [{row['ci_lower']:.4f}, {row['ci_upper']:.4f}]\")\n",
    "    \n",
    "    # Temperature effects bootstrap\n",
    "    temp_ci_results = bootstrap_temperature_effects(df, n_bootstrap=ANALYSIS_CONFIG['n_bootstrap'])\n",
    "    \n",
    "    if not temp_ci_results.empty:\n",
    "        print(f\"\\nTEMPERATURE EFFECT CONFIDENCE INTERVALS:\")\n",
    "        for _, row in temp_ci_results.head(10).iterrows():\n",
    "            print(f\"  {row['metric']}: r = {row['mean_correlation']:.4f} \"\n",
    "                  f\"[{row['ci_lower']:.4f}, {row['ci_upper']:.4f}]\")\n",
    "    \n",
    "    # Effect sizes\n",
    "    effect_sizes = compute_effect_sizes(df)\n",
    "    \n",
    "    if not effect_sizes.empty:\n",
    "        print(f\"\\nEFFECT SIZES:\")\n",
    "        for _, row in effect_sizes.head(10).iterrows():\n",
    "            print(f\"  {row['metric']}: Cohen's d = {row['cohens_d']:.4f} ({row['interpretation']})\")\n",
    "        \n",
    "        # Save effect sizes\n",
    "        effect_file = os.path.join(DIRS['bootstrap'], 'effect_sizes.csv')\n",
    "        effect_sizes.to_csv(effect_file, index=False)\n",
    "    \n",
    "    # Semantic coherence bootstrap (if available)\n",
    "    if 'bertscore_df' in locals() and not bertscore_df.empty:\n",
    "        semantic_bootstrap = bootstrap_semantic_coherence(bertscore_df, n_bootstrap=500)  # Reduced for efficiency\n",
    "        \n",
    "        if not semantic_bootstrap.empty:\n",
    "            print(f\"\\nSEMANTIC COHERENCE BOOTSTRAP:\")\n",
    "            semantic_summary = semantic_bootstrap.groupby(['analysis_type', 'prompt_type'])['mean_f1'].agg(['mean', 'std'])\n",
    "            print(semantic_summary)\n",
    "            \n",
    "            # Save semantic bootstrap results\n",
    "            semantic_bootstrap_file = os.path.join(DIRS['bootstrap'], 'semantic_coherence_bootstrap.csv')\n",
    "            semantic_bootstrap.to_csv(semantic_bootstrap_file, index=False)\n",
    "    \n",
    "    print(f\"\\nBootstrap analysis completed!\")\n",
    "    print(f\"Results saved in: {DIRS['bootstrap']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No network metrics data available for bootstrap analysis\")\n",
    "    print(\"Please run network analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ca1db",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "The `NetworkVisualizer` class creates comprehensive visualizations of the analysis results. This includes:\n",
    "\n",
    "- **Overview plots**: Distribution of network metrics across conditions\n",
    "- **Correlation heatmaps**: Relationships between different network properties\n",
    "- **Comparative analysis**: Differences between prompt types and temperatures\n",
    "- **Dimensionality reduction**: PCA plots for pattern identification\n",
    "- **Clustering analysis**: Visualization of network groupings\n",
    "- **Statistical summaries**: Box plots and violin plots for metric distributions\n",
    "\n",
    "These visualizations help interpret the results and identify patterns in how different generation conditions affect network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1227ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network visualizer...\n",
      "Loaded network metrics for visualization: 14 networks\n",
      "Creating overview plots...\n",
      "Overview plots created: figures/network_overview.png\n",
      "Creating correlation analysis...\n",
      "Error creating correlation heatmap: 'NetworkVisualizer' object has no attribute 'create_correlation_heatmap'\n",
      "Creating comparative analysis...\n",
      "Error creating comparative analysis: 'NetworkVisualizer' object has no attribute 'create_comparative_analysis'\n",
      "\n",
      "Available visualizations:\n",
      "  - network_overview.png\n",
      "\n",
      "Visualization summary:\n",
      "  - Total networks visualized: 14\n",
      "  - Prompt types: complex, vague\n",
      "  - Temperature range: 0.001 - 1.5\n",
      "  - Figures created: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize visualizer\n",
    "print(\"Initializing network visualizer...\")\n",
    "visualizer = NetworkVisualizer(\n",
    "    style=ANALYSIS_CONFIG['style'],\n",
    "    figsize=ANALYSIS_CONFIG['figsize']\n",
    ")\n",
    "\n",
    "# Check if we have data to visualize\n",
    "results_file = os.path.join(DIRS['results'], 'network_metrics.csv')\n",
    "\n",
    "if not os.path.exists(results_file):\n",
    "    print(\"Network metrics file not found. Please run network analysis first.\")\n",
    "else:\n",
    "    # Load network metrics\n",
    "    df = pd.read_csv(results_file)\n",
    "    print(f\"Loaded network metrics for visualization: {len(df)} networks\")\n",
    "    \n",
    "    # Create overview plots\n",
    "    print(\"Creating overview plots...\")\n",
    "    overview_path = os.path.join(DIRS['figures'], 'network_overview.png')\n",
    "    \n",
    "    if os.path.exists(overview_path):\n",
    "        print(f\"Overview plots already exist: {overview_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            saved_path = visualizer.create_overview_plots(df, save_path=overview_path)\n",
    "            print(f\"Overview plots created: {saved_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating overview plots: {e}\")\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    print(\"Creating correlation analysis...\")\n",
    "    correlation_path = os.path.join(DIRS['figures'], 'correlation_heatmap.png')\n",
    "    \n",
    "    if os.path.exists(correlation_path):\n",
    "        print(f\"Correlation heatmap already exists: {correlation_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            saved_path = visualizer.create_correlation_heatmap(df, save_path=correlation_path)\n",
    "            print(f\"Correlation heatmap created: {saved_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating correlation heatmap: {e}\")\n",
    "    \n",
    "    # Create comparative analysis\n",
    "    print(\"Creating comparative analysis...\")\n",
    "    comparison_path = os.path.join(DIRS['figures'], 'comparative_analysis.png')\n",
    "    \n",
    "    if os.path.exists(comparison_path):\n",
    "        print(f\"Comparative analysis already exists: {comparison_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            saved_path = visualizer.create_comparative_analysis(df, save_path=comparison_path)\n",
    "            print(f\"Comparative analysis created: {saved_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating comparative analysis: {e}\")\n",
    "    \n",
    "    # List all created visualizations\n",
    "    print(\"\\nAvailable visualizations:\")\n",
    "    viz_files = glob.glob(os.path.join(DIRS['figures'], '*.png'))\n",
    "    for viz_file in sorted(viz_files):\n",
    "        print(f\"  - {os.path.basename(viz_file)}\")\n",
    "    \n",
    "    # Show basic visualization statistics\n",
    "    print(f\"\\nVisualization summary:\")\n",
    "    print(f\"  - Total networks visualized: {len(df)}\")\n",
    "    print(f\"  - Prompt types: {', '.join(df['prompt_type'].unique())}\")\n",
    "    print(f\"  - Temperature range: {df['temperature'].min()} - {df['temperature'].max()}\")\n",
    "    print(f\"  - Figures created: {len(viz_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80746506",
   "metadata": {},
   "source": [
    "## 9. Results Summary and Export\n",
    "\n",
    "This final section provides a comprehensive summary of the analysis and exports the results for further use. The pipeline generates several output files:\n",
    "\n",
    "- **Network metrics CSV**: Quantitative measures for each network\n",
    "- **Advanced analysis results**: Statistical test results and model outputs\n",
    "- **Visualization files**: Plots and figures for interpretation\n",
    "- **Detailed text reports**: Summary of findings and interpretations\n",
    "\n",
    "The results can be used for:\n",
    "- Research publications and presentations\n",
    "- Further statistical analysis\n",
    "- Comparison with other text generation models\n",
    "- Understanding the impact of generation parameters on semantic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e536bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEMANTIC ANALYSIS PIPELINE - RESULTS SUMMARY\n",
      "\n",
      "TEXT GENERATION & PREPROCESSING:\n",
      "complex_original: 7 files\n",
      "vague_original: 7 files\n",
      "complex_cleaned: 7 files\n",
      "vague_cleaned: 7 files\n",
      "\n",
      "SEMANTIC NETWORKS:\n",
      "Complex networks: 7 files\n",
      "Vague networks: 7 files\n",
      "\n",
      "ANALYSIS RESULTS:\n",
      "network_metrics: 14 records\n",
      "advanced_analysis: Not found\n",
      "detailed_results: Not found\n",
      "\n",
      "VISUALIZATIONS:\n",
      "Total figures: 1\n",
      "    - network_overview.png\n",
      "\n",
      "PIPELINE STATUS:\n",
      "Completion rate: 100.0%\n",
      "Completed components: 4/4\n",
      "Pipeline completed successfully!\n",
      "\n",
      "Summary report saved to: results/pipeline_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive results summary\n",
    "print(\"SEMANTIC ANALYSIS PIPELINE - RESULTS SUMMARY\")\n",
    "\n",
    "# Check all output files and directories\n",
    "output_summary = {}\n",
    "\n",
    "# 1. Generated texts - Updated to match existing directory structure\n",
    "text_dirs = {\n",
    "    'complex_original': os.path.join(DIRS['texts'], 'mistral_complex'),\n",
    "    'vague_original': os.path.join(DIRS['texts'], 'mistral_vague'),\n",
    "    'complex_cleaned': os.path.join(DIRS['texts'], 'cleaned_mistral_complex'),\n",
    "    'vague_cleaned': os.path.join(DIRS['texts'], 'cleaned_mistral_vague')\n",
    "}\n",
    "\n",
    "print(\"\\nTEXT GENERATION & PREPROCESSING:\")\n",
    "for name, dir_path in text_dirs.items():\n",
    "    if os.path.exists(dir_path):\n",
    "        count = len(glob.glob(os.path.join(dir_path, '*.txt')))\n",
    "        output_summary[name] = count\n",
    "        print(f\"{name}: {count} files\")\n",
    "    else:\n",
    "        output_summary[name] = 0\n",
    "        print(f\"{name}: Directory not found\")\n",
    "\n",
    "# 2. Network edge lists\n",
    "edge_dirs = ['emo_edges_complex', 'emo_edges_vague']\n",
    "print(\"\\nSEMANTIC NETWORKS:\")\n",
    "for edge_dir in edge_dirs:\n",
    "    if os.path.exists(edge_dir):\n",
    "        count = len(glob.glob(os.path.join(edge_dir, '*.txt')))\n",
    "        output_summary[edge_dir] = count\n",
    "        network_type = 'complex' if 'complex' in edge_dir else 'vague'\n",
    "        print(f\"{network_type.capitalize()} networks: {count} files\")\n",
    "    else:\n",
    "        output_summary[edge_dir] = 0\n",
    "        print(f\"{edge_dir}: Directory not found\")\n",
    "\n",
    "# 3. Analysis results\n",
    "results_files = {\n",
    "    'network_metrics': os.path.join(DIRS['results'], 'network_metrics.csv'),\n",
    "    'advanced_analysis': os.path.join(DIRS['results'], 'advanced_analysis_no_statsmodels.csv'),\n",
    "    'detailed_results': os.path.join(DIRS['results'], 'detailed_results.txt')\n",
    "}\n",
    "\n",
    "print(\"\\nANALYSIS RESULTS:\")\n",
    "for name, file_path in results_files.items():\n",
    "    if os.path.exists(file_path):\n",
    "        if file_path.endswith('.csv'):\n",
    "            df_temp = pd.read_csv(file_path)\n",
    "            count = len(df_temp)\n",
    "            output_summary[name] = count\n",
    "            print(f\"{name}: {count} records\")\n",
    "        else:\n",
    "            output_summary[name] = \"Available\"\n",
    "            print(f\"{name}: Available\")\n",
    "    else:\n",
    "        output_summary[name] = \"Not found\"\n",
    "        print(f\"{name}: Not found\")\n",
    "\n",
    "# 4. Visualizations\n",
    "viz_files = glob.glob(os.path.join(DIRS['figures'], '*.png'))\n",
    "print(f\"\\nVISUALIZATIONS:\")\n",
    "output_summary['visualizations'] = len(viz_files)\n",
    "if viz_files:\n",
    "    print(f\"Total figures: {len(viz_files)}\")\n",
    "    for viz_file in sorted(viz_files):\n",
    "        print(f\"    - {os.path.basename(viz_file)}\")\n",
    "else:\n",
    "    print(\"No visualization files found\")\n",
    "\n",
    "# 5. Pipeline completion status\n",
    "print(f\"\\nPIPELINE STATUS:\")\n",
    "required_components = ['complex_original', 'vague_original', 'network_metrics', 'visualizations']\n",
    "completed_components = [comp for comp in required_components if output_summary.get(comp, 0) > 0]\n",
    "completion_rate = len(completed_components) / len(required_components) * 100\n",
    "\n",
    "print(f\"Completion rate: {completion_rate:.1f}%\")\n",
    "print(f\"Completed components: {len(completed_components)}/{len(required_components)}\")\n",
    "\n",
    "if completion_rate == 100:\n",
    "    print(\"Pipeline completed successfully!\")\n",
    "else:\n",
    "    missing = [comp for comp in required_components if comp not in completed_components]\n",
    "    print(f\"Missing components: {', '.join(missing)}\")\n",
    "\n",
    "# 6. Export summary to file\n",
    "summary_file = os.path.join(DIRS['results'], 'pipeline_summary.txt')\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"SEMANTIC ANALYSIS PIPELINE - SUMMARY REPORT\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    \n",
    "    f.write(\"CONFIGURATION:\\n\")\n",
    "    f.write(f\"- Temperature values: {TEMPERATURES}\\n\")\n",
    "    f.write(f\"- Prompt types: {list(PROMPTS.keys())}\\n\")\n",
    "    f.write(f\"- Analysis parameters: {ANALYSIS_CONFIG}\\n\\n\")\n",
    "    \n",
    "    f.write(\"OUTPUT SUMMARY:\\n\")\n",
    "    for component, value in output_summary.items():\n",
    "        f.write(f\"- {component}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nPipeline completion: {completion_rate:.1f}%\\n\")\n",
    "\n",
    "print(f\"\\nSummary report saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72978d",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Next Steps\n",
    "\n",
    "This notebook demonstrates a complete pipeline for semantic analysis of texts generated with Mistral AI. The modular approach using separate Python files provides several advantages:\n",
    "\n",
    "### Key Benefits:\n",
    "- **Modularity**: Each component can be developed and tested independently\n",
    "- **Reusability**: Components can be used in other projects\n",
    "- **Maintainability**: Code is organized and easier to debug\n",
    "- **Scalability**: Pipeline can be extended with new analysis methods\n",
    "\n",
    "### Pipeline Components:\n",
    "1. **Text Generation**: Systematic generation with different parameters\n",
    "2. **Preprocessing**: Standardized text cleaning and normalization\n",
    "3. **Network Construction**: Semantic relationship mapping\n",
    "4. **Analysis**: Comprehensive statistical evaluation\n",
    "5. **Visualization**: Clear presentation of results\n",
    "\n",
    "### Potential Extensions:\n",
    "- Integration with other language models (GPT, Claude, etc.)\n",
    "- Additional network analysis methods\n",
    "- Real-time analysis capabilities\n",
    "- Web interface for interactive exploration\n",
    "- Comparative studies across different models\n",
    "\n",
    "### Research Applications:\n",
    "- Understanding creativity in AI-generated texts\n",
    "- Analyzing semantic coherence across generation parameters\n",
    "- Studying the impact of prompt design on output structure\n",
    "- Developing metrics for text quality assessment\n",
    "\n",
    "The pipeline is now ready for production use and can be easily adapted for different research questions and datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hakan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
